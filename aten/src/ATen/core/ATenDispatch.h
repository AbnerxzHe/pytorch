#pragma once

#include <c10/core/Backend.h>
#include <unordered_map>

namespace at {

template<class FuncType>
class CAFFE2_API ATenOp {};
template<class Return, class... Params>
class CAFFE2_API ATenOp<Return (Params...)> {
  using FnPtr = Return (*)(Params...);
  using WrapperPtr = Return (*)(FnPtr, Params...);
 public:
  Return operator()(Params... params) const {
    if (wrapper_) {
      return (*wrapper_)(fn_, params...);
    }
    return (*fn_)(params...);
  }

 private:
  explicit ATenOp(void* fn)
  : fn_(reinterpret_cast<FnPtr>(fn)), wrapper_(nullptr) {}
  explicit ATenOp(void* fn, void* wrapper)
  : fn_(reinterpret_cast<FnPtr>(fn)), wrapper_(reinterpret_cast<WrapperPtr>(wrapper)) {}
  friend class ATenDispatch;

  FnPtr fn_;
  WrapperPtr wrapper_;
};

class CAFFE2_API ATenDispatch {
 public:
  template<class FuncType>
  ATenDispatch& registerOp(Backend backend, const char* schema, FuncType* fn) {
   auto id = getSchemaId(schema);
   getFunctionTable(backend)[id] = reinterpret_cast<void*>(fn);
   return *this;
  }

  template <typename WrapperFuncType>
  ATenDispatch& registerVariableWrapper(const char* schema, WrapperFuncType* fn) {
    auto id = getSchemaId(schema);
    getWrapperTable()[id] = reinterpret_cast<void*>(fn);
    return *this;
  }

  template<class FuncType>
  ATenOp<FuncType> getOp(Backend backend, bool is_variable, int64_t id, const std::string& name) {
    void** function_table = getFunctionTable(backend);
    void** default_function_table = getFunctionTable(Backend::Undefined);
    void** wrapper_table = getWrapperTable();

    if (function_table[id] == nullptr) {
      if (default_function_table[id] == nullptr) {
        AT_ERROR("No function is registered for ", name, " on backend ", toString(backend));
      }
      function_table[id] = default_function_table[id];
    }

    if (is_variable) {
      if (wrapper_table[id] == nullptr) {
        AT_ERROR("No autograd wrapper is registered for ", name, ". Please report a bug to PyTorch.");
      }
      return ATenOp<FuncType>(function_table[id], wrapper_table[id]);
    }
    return ATenOp<FuncType>(function_table[id]);
  }

 private:
  void initCuda();

  int64_t getSchemaId(std::string schema) {
    static std::unordered_map<std::string, int64_t> schema_to_id = {
      {"aten::_cast_Byte(Tensor self, bool non_blocking=False) -> Tensor", 0},
      {"aten::_cast_Char(Tensor self, bool non_blocking=False) -> Tensor", 1},
      {"aten::_cast_Double(Tensor self, bool non_blocking=False) -> Tensor", 2},
      {"aten::_cast_Float(Tensor self, bool non_blocking=False) -> Tensor", 3},
      {"aten::_cast_Int(Tensor self, bool non_blocking=False) -> Tensor", 4},
      {"aten::_cast_Long(Tensor self, bool non_blocking=False) -> Tensor", 5},
      {"aten::_cast_Short(Tensor self, bool non_blocking=False) -> Tensor", 6},
      {"aten::_cast_Half(Tensor self, bool non_blocking=False) -> Tensor", 7},
      {"aten::_cudnn_ctc_loss(Tensor log_probs, Tensor targets, int[] input_lengths, int[] target_lengths, int blank, bool deterministic, bool zero_infinity) -> (Tensor, Tensor)", 8},
      {"aten::_cudnn_rnn_flatten_weight(Tensor[] weight_arr, int weight_stride0, int input_size, int mode, int hidden_size, int num_layers, bool batch_first, bool bidirectional) -> Tensor", 9},
      {"aten::_cudnn_rnn(Tensor input, Tensor[] weight, int weight_stride0, Tensor? weight_buf, Tensor hx, Tensor? cx, int mode, int hidden_size, int num_layers, bool batch_first, float dropout, bool train, bool bidirectional, int[] batch_sizes, Tensor? dropout_state) -> (Tensor, Tensor, Tensor, Tensor, Tensor)", 10},
      {"aten::_cudnn_rnn_backward(Tensor input, Tensor[] weight, int weight_stride0, Tensor weight_buf, Tensor hx, Tensor? cx, Tensor output, Tensor? grad_output, Tensor? grad_hy, Tensor? grad_cy, int mode, int hidden_size, int num_layers, bool batch_first, float dropout, bool train, bool bidirectional, int[] batch_sizes, Tensor? dropout_state, Tensor reserve, bool[4] output_mask) -> (Tensor, Tensor, Tensor, Tensor[])", 11},
      {"aten::_cudnn_init_dropout_state(float dropout, bool train, int dropout_seed, *, ScalarType dtype, Layout layout, Device device, bool pin_memory=False) -> Tensor", 12},
      {"aten::_debug_has_internal_overlap(Tensor self) -> int", 13},
      {"aten::_fused_dropout(Tensor self, float p, Generator? generator=None) -> (Tensor, Tensor)", 14},
      {"aten::_masked_scale(Tensor self, Tensor mask, float scale) -> Tensor", 15},
      {"aten::_sobol_engine_draw(Tensor quasi, int n, Tensor sobolstate, int dimension, int num_generated, ScalarType? dtype) -> (Tensor, Tensor)", 16},
      {"aten::_sobol_engine_ff_(Tensor(a!) self, int n, Tensor sobolstate, int dimension, int num_generated) -> Tensor(a!)", 17},
      {"aten::_sobol_engine_scramble_(Tensor(a!) self, Tensor ltm, int dimension) -> Tensor(a!)", 18},
      {"aten::_sobol_engine_initialize_state_(Tensor(a!) self, int dimension) -> Tensor(a!)", 19},
      {"aten::_reshape_from_tensor(Tensor self, Tensor shape) -> Tensor", 20},
      {"aten::_shape_as_tensor(Tensor self) -> Tensor", 21},
      {"aten::dropout(Tensor input, float p, bool train) -> Tensor", 22},
      {"aten::dropout_(Tensor(a!) self, float p, bool train) -> Tensor(a!)", 23},
      {"aten::feature_dropout(Tensor input, float p, bool train) -> Tensor", 24},
      {"aten::feature_dropout_(Tensor(a!) self, float p, bool train) -> Tensor(a!)", 25},
      {"aten::alpha_dropout(Tensor input, float p, bool train) -> Tensor", 26},
      {"aten::alpha_dropout_(Tensor(a!) self, float p, bool train) -> Tensor(a!)", 27},
      {"aten::feature_alpha_dropout(Tensor input, float p, bool train) -> Tensor", 28},
      {"aten::feature_alpha_dropout_(Tensor(a!) self, float p, bool train) -> Tensor(a!)", 29},
      {"aten::abs(Tensor self) -> Tensor", 30},
      {"aten::abs_(Tensor(a!) self) -> Tensor(a!)", 31},
      {"aten::abs(Tensor self, *, Tensor(a!) out) -> Tensor(a!)", 32},
      {"aten::acos(Tensor self) -> Tensor", 33},
      {"aten::acos_(Tensor(a!) self) -> Tensor(a!)", 34},
      {"aten::acos(Tensor self, *, Tensor(a!) out) -> Tensor(a!)", 35},
      {"aten::avg_pool1d(Tensor self, int[1] kernel_size, int[1] stride=[], int[1] padding=0, bool ceil_mode=False, bool count_include_pad=True) -> Tensor", 36},
      {"aten::adaptive_avg_pool1d(Tensor self, int[1] output_size) -> Tensor", 37},
      {"aten::adaptive_max_pool1d(Tensor self, int[1] output_size) -> (Tensor, Tensor)", 38},
      {"aten::add(Tensor self, Tensor other, *, Scalar alpha=1) -> Tensor", 39},
      {"aten::add_(Tensor(a!) self, Tensor other, *, Scalar alpha=1) -> Tensor(a!)", 40},
      {"aten::add(Tensor self, Tensor other, *, Scalar alpha=1, Tensor(a!) out) -> Tensor(a!)", 41},
      {"aten::add(Tensor self, Scalar other, Scalar alpha=1) -> Tensor", 42},
      {"aten::add_(Tensor(a!) self, Scalar other, Scalar alpha=1) -> Tensor(a!)", 43},
      {"aten::addmv(Tensor self, Tensor mat, Tensor vec, *, Scalar beta=1, Scalar alpha=1) -> Tensor", 44},
      {"aten::addmv_(Tensor(a!) self, Tensor mat, Tensor vec, *, Scalar beta=1, Scalar alpha=1) -> Tensor(a!)", 45},
      {"aten::addmv(Tensor self, Tensor mat, Tensor vec, *, Scalar beta=1, Scalar alpha=1, Tensor(a!) out) -> Tensor(a!)", 46},
      {"aten::addr(Tensor self, Tensor vec1, Tensor vec2, *, Scalar beta=1, Scalar alpha=1) -> Tensor", 47},
      {"aten::addr_(Tensor(a!) self, Tensor vec1, Tensor vec2, *, Scalar beta=1, Scalar alpha=1) -> Tensor(a!)", 48},
      {"aten::addr(Tensor self, Tensor vec1, Tensor vec2, *, Scalar beta=1, Scalar alpha=1, Tensor(a!) out) -> Tensor(a!)", 49},
      {"aten::affine_grid_generator(Tensor theta, int[] size) -> Tensor", 50},
      {"aten::affine_grid_generator_backward(Tensor grad, int[] size) -> Tensor", 51},
      {"aten::all(Tensor self, int dim, bool keepdim=False) -> Tensor", 52},
      {"aten::all(Tensor self, int dim, bool keepdim=False, *, Tensor(a!) out) -> Tensor(a!)", 53},
      {"aten::allclose(Tensor self, Tensor other, float rtol=1e-05, float atol=1e-08, bool equal_nan=False) -> bool", 54},
      {"aten::any(Tensor self, int dim, bool keepdim=False) -> Tensor", 55},
      {"aten::any(Tensor self, int dim, bool keepdim=False, *, Tensor(a!) out) -> Tensor(a!)", 56},
      {"aten::arange(Scalar end, *, ScalarType? dtype=None, Layout? layout=None, Device? device=None, bool? pin_memory=None) -> Tensor", 57},
      {"aten::arange(Scalar start, Scalar end, *, ScalarType? dtype=None, Layout? layout=None, Device? device=None, bool? pin_memory=None) -> Tensor", 58},
      {"aten::arange(Scalar start, Scalar end, Scalar step, *, ScalarType? dtype=None, Layout? layout=None, Device? device=None, bool? pin_memory=None) -> Tensor", 59},
      {"aten::arange(Scalar end, *, Tensor(a!) out) -> Tensor(a!)", 60},
      {"aten::arange(Scalar start, Scalar end, Scalar step=1, *, Tensor(a!) out) -> Tensor(a!)", 61},
      {"aten::_dim_arange(Tensor like, int dim) -> Tensor", 62},
      {"aten::argmax(Tensor self, int? dim=None, bool keepdim=False) -> Tensor", 63},
      {"aten::argmin(Tensor self, int? dim=None, bool keepdim=False) -> Tensor", 64},
      {"aten::as_strided(Tensor(a) self, int[] size, int[] stride, int? storage_offset=None) -> Tensor(a)", 65},
      {"aten::as_strided_(Tensor(a!) self, int[] size, int[] stride, int? storage_offset=None) -> Tensor(a!)", 66},
      {"aten::asin(Tensor self) -> Tensor", 67},
      {"aten::asin_(Tensor(a!) self) -> Tensor(a!)", 68},
      {"aten::asin(Tensor self, *, Tensor(a!) out) -> Tensor(a!)", 69},
      {"aten::atan(Tensor self) -> Tensor", 70},
      {"aten::atan_(Tensor(a!) self) -> Tensor(a!)", 71},
      {"aten::atan(Tensor self, *, Tensor(a!) out) -> Tensor(a!)", 72},
      {"aten::baddbmm(Tensor self, Tensor batch1, Tensor batch2, *, Scalar beta=1, Scalar alpha=1) -> Tensor", 73},
      {"aten::baddbmm_(Tensor(a!) self, Tensor batch1, Tensor batch2, *, Scalar beta=1, Scalar alpha=1) -> Tensor(a!)", 74},
      {"aten::_baddbmm_mkl_(Tensor(a!) self, Tensor batch1, Tensor batch2, *, Scalar beta=1, Scalar alpha=1) -> Tensor(a!)", 75},
      {"aten::baddbmm(Tensor self, Tensor batch1, Tensor batch2, *, Scalar beta=1, Scalar alpha=1, Tensor(a!) out) -> Tensor(a!)", 76},
      {"aten::bartlett_window(int window_length, *, ScalarType? dtype=None, Layout? layout=None, Device? device=None, bool? pin_memory=None) -> Tensor", 77},
      {"aten::bartlett_window(int window_length, bool periodic, *, ScalarType? dtype=None, Layout? layout=None, Device? device=None, bool? pin_memory=None) -> Tensor", 78},
      {"aten::batch_norm(Tensor input, Tensor? weight, Tensor? bias, Tensor? running_mean, Tensor? running_var, bool training, float momentum, float eps, bool cudnn_enabled) -> Tensor", 79},
      {"aten::_batch_norm_impl_index(Tensor input, Tensor? weight, Tensor? bias, Tensor? running_mean, Tensor? running_var, bool training, float momentum, float eps, bool cudnn_enabled) -> (Tensor, Tensor, Tensor, int)", 80},
      {"aten::_batch_norm_impl_index_backward(int impl_index, Tensor input, Tensor grad_output, Tensor? weight, Tensor? running_mean, Tensor? running_var, Tensor? save_mean, Tensor? save_var_transform, bool train, float eps, bool[3] output_mask) -> (Tensor, Tensor, Tensor)", 81},
      {"aten::bernoulli(Tensor self, *, Generator? generator=None) -> Tensor", 82},
      {"aten::bernoulli(Tensor self, *, Generator? generator=None, Tensor(a!) out) -> Tensor(a!)", 83},
      {"aten::bernoulli_(Tensor(a!) self, Tensor p, *, Generator? generator=None) -> Tensor(a!)", 84},
      {"aten::bernoulli_(Tensor(a!) self, float p=0.5, *, Generator? generator=None) -> Tensor(a!)", 85},
      {"aten::bernoulli(Tensor self, float p, *, Generator? generator=None) -> Tensor", 86},
      {"aten::bilinear(Tensor input1, Tensor input2, Tensor weight, Tensor? bias) -> Tensor", 87},
      {"aten::binary_cross_entropy_with_logits(Tensor self, Tensor target, Tensor? weight, Tensor? pos_weight, int reduction) -> Tensor", 88},
      {"aten::binary_cross_entropy_with_logits_backward(Tensor grad_output, Tensor self, Tensor target, Tensor? weight, Tensor? pos_weight, int reduction) -> Tensor", 89},
      {"aten::bincount(Tensor self, Tensor? weights=None, int minlength=0) -> Tensor", 90},
      {"aten::blackman_window(int window_length, *, ScalarType? dtype=None, Layout? layout=None, Device? device=None, bool? pin_memory=None) -> Tensor", 91},
      {"aten::blackman_window(int window_length, bool periodic, *, ScalarType? dtype=None, Layout? layout=None, Device? device=None, bool? pin_memory=None) -> Tensor", 92},
      {"aten::bmm(Tensor self, Tensor mat2) -> Tensor", 93},
      {"aten::bmm(Tensor self, Tensor mat2, *, Tensor(a!) out) -> Tensor(a!)", 94},
      {"aten::broadcast_tensors(Tensor[] tensors) -> Tensor[]", 95},
      {"aten::cat(Tensor[] tensors, int dim=0) -> Tensor", 96},
      {"aten::cat(Tensor[] tensors, int dim=0, *, Tensor(a!) out) -> Tensor(a!)", 97},
      {"aten::ceil(Tensor self) -> Tensor", 98},
      {"aten::ceil_(Tensor(a!) self) -> Tensor(a!)", 99},
      {"aten::ceil(Tensor self, *, Tensor(a!) out) -> Tensor(a!)", 100},
      {"aten::chain_matmul(Tensor[] matrices) -> Tensor", 101},
      {"aten::chunk(Tensor(a) self, int chunks, int dim=0) -> Tensor(a)[]", 102},
      {"aten::clamp(Tensor self, Scalar? min=None, Scalar? max=None) -> Tensor", 103},
      {"aten::clamp_(Tensor(a!) self, Scalar? min=None, Scalar? max=None) -> Tensor(a!)", 104},
      {"aten::clamp(Tensor self, Scalar? min=None, Scalar? max=None, *, Tensor(a!) out) -> Tensor(a!)", 105},
      {"aten::clamp_max(Tensor self, Scalar max) -> Tensor", 106},
      {"aten::clamp_max_(Tensor(a!) self, Scalar max) -> Tensor(a!)", 107},
      {"aten::clamp_max(Tensor self, Scalar max, *, Tensor(a!) out) -> Tensor(a!)", 108},
      {"aten::clamp_min(Tensor self, Scalar min) -> Tensor", 109},
      {"aten::clamp_min_(Tensor(a!) self, Scalar min) -> Tensor(a!)", 110},
      {"aten::clamp_min(Tensor self, Scalar min, *, Tensor(a!) out) -> Tensor(a!)", 111},
      {"aten::cudnn_is_acceptable(Tensor self) -> bool", 112},
      {"aten::constant_pad_nd(Tensor self, int[] pad, Scalar value=0) -> Tensor", 113},
      {"aten::contiguous(Tensor self, *, MemoryFormat memory_format=contiguous_format) -> Tensor", 114},
      {"aten::convolution(Tensor input, Tensor weight, Tensor? bias, int[] stride, int[] padding, int[] dilation, bool transposed, int[] output_padding, int groups) -> Tensor", 115},
      {"aten::_convolution(Tensor input, Tensor weight, Tensor? bias, int[] stride, int[] padding, int[] dilation, bool transposed, int[] output_padding, int groups, bool benchmark, bool deterministic, bool cudnn_enabled) -> Tensor", 116},
      {"aten::_convolution_nogroup(Tensor input, Tensor weight, Tensor? bias, int[] stride, int[] padding, int[] dilation, bool transposed, int[] output_padding) -> Tensor", 117},
      {"aten::_convolution_double_backward(Tensor? ggI, Tensor? ggW, Tensor? ggb, Tensor gO, Tensor weight, Tensor self, int[] stride, int[] padding, int[] dilation, bool transposed, int[] output_padding, int groups, bool benchmark, bool deterministic, bool cudnn_enabled, bool[3] output_mask) -> (Tensor, Tensor, Tensor)", 118},
      {"aten::conv1d(Tensor input, Tensor weight, Tensor? bias=None, int[1] stride=1, int[1] padding=0, int[1] dilation=1, int groups=1) -> Tensor", 119},
      {"aten::conv2d(Tensor input, Tensor weight, Tensor? bias=None, int[2] stride=1, int[2] padding=0, int[2] dilation=1, int groups=1) -> Tensor", 120},
      {"aten::conv3d(Tensor input, Tensor weight, Tensor? bias=None, int[3] stride=1, int[3] padding=0, int[3] dilation=1, int groups=1) -> Tensor", 121},
      {"aten::conv_tbc(Tensor self, Tensor weight, Tensor bias, int pad=0) -> Tensor", 122},
      {"aten::conv_tbc_backward(Tensor self, Tensor input, Tensor weight, Tensor bias, int pad) -> (Tensor, Tensor, Tensor)", 123},
      {"aten::conv_transpose1d(Tensor input, Tensor weight, Tensor? bias=None, int[1] stride=1, int[1] padding=0, int[1] output_padding=0, int groups=1, int[1] dilation=1) -> Tensor", 124},
      {"aten::conv_transpose2d(Tensor input, Tensor weight, Tensor? bias=None, int[2] stride=1, int[2] padding=0, int[2] output_padding=0, int groups=1, int[2] dilation=1) -> Tensor", 125},
      {"aten::conv_transpose3d(Tensor input, Tensor weight, Tensor? bias=None, int[3] stride=1, int[3] padding=0, int[3] output_padding=0, int groups=1, int[3] dilation=1) -> Tensor", 126},
      {"aten::copy_(Tensor(a!) self, Tensor src, bool non_blocking=False) -> Tensor(a!)", 127},
      {"aten::_copy_from(Tensor self, Tensor dst, bool non_blocking=False) -> Tensor", 128},
      {"aten::cos(Tensor self) -> Tensor", 129},
      {"aten::cos_(Tensor(a!) self) -> Tensor(a!)", 130},
      {"aten::cos(Tensor self, *, Tensor(a!) out) -> Tensor(a!)", 131},
      {"aten::cosh(Tensor self) -> Tensor", 132},
      {"aten::cosh_(Tensor(a!) self) -> Tensor(a!)", 133},
      {"aten::cosh(Tensor self, *, Tensor(a!) out) -> Tensor(a!)", 134},
      {"aten::cosine_embedding_loss(Tensor input1, Tensor input2, Tensor target, float margin=0.0, int reduction=Mean) -> Tensor", 135},
      {"aten::cudnn_affine_grid_generator(Tensor theta, int N, int C, int H, int W) -> Tensor grid", 136},
      {"aten::cudnn_affine_grid_generator_backward(Tensor grad, int N, int C, int H, int W) -> Tensor grad_theta", 137},
      {"aten::cudnn_batch_norm(Tensor input, Tensor weight, Tensor? bias, Tensor? running_mean, Tensor? running_var, bool training, float exponential_average_factor, float epsilon) -> (Tensor, Tensor, Tensor)", 138},
      {"aten::cudnn_batch_norm_backward(Tensor input, Tensor grad_output, Tensor weight, Tensor? running_mean, Tensor? running_var, Tensor? save_mean, Tensor? save_var, float epsilon) -> (Tensor, Tensor, Tensor)", 139},
      {"aten::cudnn_convolution(Tensor self, Tensor weight, Tensor? bias, int[] padding, int[] stride, int[] dilation, int groups, bool benchmark, bool deterministic) -> Tensor", 140},
      {"aten::cudnn_convolution_backward_input(int[] self_size, Tensor grad_output, Tensor weight, int[] padding, int[] stride, int[] dilation, int groups, bool benchmark, bool deterministic) -> Tensor", 141},
      {"aten::cudnn_convolution_backward(Tensor self, Tensor grad_output, Tensor weight, int[] padding, int[] stride, int[] dilation, int groups, bool benchmark, bool deterministic, bool[3] output_mask) -> (Tensor, Tensor, Tensor)", 142},
      {"aten::cudnn_convolution_backward_bias(Tensor grad_output) -> Tensor", 143},
      {"aten::cudnn_convolution_backward_weight(int[] weight_size, Tensor grad_output, Tensor self, int[] padding, int[] stride, int[] dilation, int groups, bool benchmark, bool deterministic) -> Tensor", 144},
      {"aten::cudnn_convolution_transpose(Tensor self, Tensor weight, Tensor? bias, int[] padding, int[] output_padding, int[] stride, int[] dilation, int groups, bool benchmark, bool deterministic) -> Tensor", 145},
      {"aten::cudnn_convolution_transpose_backward(Tensor self, Tensor grad_output, Tensor weight, int[] padding, int[] output_padding, int[] stride, int[] dilation, int groups, bool benchmark, bool deterministic, bool[3] output_mask) -> (Tensor, Tensor, Tensor)", 146},
      {"aten::cudnn_convolution_transpose_backward_bias(Tensor grad_output) -> Tensor", 147},
      {"aten::cudnn_convolution_transpose_backward_input(Tensor grad_output, Tensor weight, int[] padding, int[] stride, int[] dilation, int groups, bool benchmark, bool deterministic) -> Tensor", 148},
      {"aten::cudnn_convolution_transpose_backward_weight(int[] weight_size, Tensor grad_output, Tensor self, int[] padding, int[] stride, int[] dilation, int groups, bool benchmark, bool deterministic) -> Tensor", 149},
      {"aten::cudnn_grid_sampler(Tensor self, Tensor grid) -> Tensor output", 150},
      {"aten::cudnn_grid_sampler_backward(Tensor self, Tensor grid, Tensor grad_output) -> (Tensor grad_self, Tensor grad_grid)", 151},
      {"aten::cumsum(Tensor self, int dim, *, ScalarType dtype) -> Tensor", 152},
      {"aten::cumsum(Tensor self, int dim) -> Tensor", 153},
      {"aten::cumsum(Tensor self, int dim, *, ScalarType dtype, Tensor(a!) out) -> Tensor(a!)", 154},
      {"aten::cumsum(Tensor self, int dim, *, Tensor(a!) out) -> Tensor(a!)", 155},
      {"aten::cumprod(Tensor self, int dim, *, ScalarType dtype) -> Tensor", 156},
      {"aten::cumprod(Tensor self, int dim) -> Tensor", 157},
      {"aten::cumprod(Tensor self, int dim, *, ScalarType dtype, Tensor(a!) out) -> Tensor(a!)", 158},
      {"aten::cumprod(Tensor self, int dim, *, Tensor(a!) out) -> Tensor(a!)", 159},
      {"aten::ctc_loss(Tensor log_probs, Tensor targets, int[] input_lengths, int[] target_lengths, int blank=0, int reduction=Mean, bool zero_infinity=False) -> Tensor", 160},
      {"aten::ctc_loss(Tensor log_probs, Tensor targets, Tensor input_lengths, Tensor target_lengths, int blank=0, int reduction=Mean, bool zero_infinity=False) -> Tensor", 161},
      {"aten::_ctc_loss(Tensor log_probs, Tensor targets, int[] input_lengths, int[] target_lengths, int blank=0, bool zero_infinity=False) -> (Tensor, Tensor)", 162},
      {"aten::_ctc_loss_backward(Tensor grad, Tensor log_probs, Tensor targets, int[] input_lengths, int[] target_lengths, Tensor neg_log_likelihood, Tensor log_alpha, int blank, bool zero_infinity=False) -> Tensor", 163},
      {"aten::det(Tensor self) -> Tensor", 164},
      {"aten::diag_embed(Tensor self, int offset=0, int dim1=-2, int dim2=-1) -> Tensor", 165},
      {"aten::diagflat(Tensor self, int offset=0) -> Tensor", 166},
      {"aten::diagonal(Tensor(a) self, int offset=0, int dim1=0, int dim2=1) -> Tensor(a)", 167},
      {"aten::div(Tensor self, Tensor other) -> Tensor", 168},
      {"aten::div_(Tensor(a!) self, Tensor other) -> Tensor(a!)", 169},
      {"aten::div(Tensor self, Tensor other, *, Tensor(a!) out) -> Tensor(a!)", 170},
      {"aten::div(Tensor self, Scalar other) -> Tensor", 171},
      {"aten::div_(Tensor(a!) self, Scalar other) -> Tensor(a!)", 172},
      {"aten::dot(Tensor self, Tensor tensor) -> Tensor", 173},
      {"aten::dot(Tensor self, Tensor tensor, *, Tensor(a!) out) -> Tensor(a!)", 174},
      {"aten::einsum(str equation, Tensor[] tensors) -> Tensor", 175},
      {"aten::embedding(Tensor weight, Tensor indices, int padding_idx=-1, bool scale_grad_by_freq=False, bool sparse=False) -> Tensor", 176},
      {"aten::embedding_backward(Tensor grad, Tensor indices, int num_weights, int padding_idx, bool scale_grad_by_freq, bool sparse) -> Tensor", 177},
      {"aten::embedding_dense_backward(Tensor grad_output, Tensor indices, int num_weights, int padding_idx, bool scale_grad_by_freq) -> Tensor", 178},
      {"aten::embedding_renorm_(Tensor(a!) self, Tensor indices, float max_norm, float norm_type) -> Tensor(a!)", 179},
      {"aten::embedding_sparse_backward(Tensor grad, Tensor indices, int num_weights, int padding_idx, bool scale_grad_by_freq) -> Tensor", 180},
      {"aten::embedding_bag(Tensor weight, Tensor indices, Tensor offsets, bool scale_grad_by_freq=False, int mode=0, bool sparse=False, Tensor? per_sample_weights=None) -> (Tensor, Tensor, Tensor, Tensor)", 181},
      {"aten::_embedding_bag(Tensor weight, Tensor indices, Tensor offsets, bool scale_grad_by_freq=False, int mode=0, bool sparse=False, Tensor? per_sample_weights=None) -> (Tensor, Tensor, Tensor, Tensor)", 182},
      {"aten::_embedding_bag_backward(Tensor grad, Tensor indices, Tensor offsets, Tensor offset2bag, Tensor bag_size, Tensor maximum_indices, int num_weights, bool scale_grad_by_freq, int mode, bool sparse, Tensor? per_sample_weights) -> Tensor", 183},
      {"aten::_embedding_bag_sparse_backward(Tensor grad, Tensor indices, Tensor offsets, Tensor offset2bag, Tensor bag_size, int num_weights, bool scale_grad_by_freq, int mode, Tensor? per_sample_weights) -> Tensor", 184},
      {"aten::_embedding_bag_dense_backward(Tensor grad, Tensor indices, Tensor offsets, Tensor offset2bag, Tensor bag_size, Tensor maximum_indices, int num_weights, bool scale_grad_by_freq, int mode, Tensor? per_sample_weights) -> Tensor", 185},
      {"aten::_embedding_bag_per_sample_weights_backward(Tensor grad, Tensor weight, Tensor indices, Tensor offsets, Tensor offset2bag, int mode) -> Tensor", 186},
      {"aten::empty(int[] size, *, ScalarType? dtype=None, Layout? layout=None, Device? device=None, bool? pin_memory=None) -> Tensor", 187},
      {"aten::_empty_affine_quantized(int[] size, *, ScalarType? dtype=None, Layout? layout=None, Device? device=None, bool? pin_memory=None, float scale=1, int zero_point=0) -> Tensor", 188},
      {"aten::resize_(Tensor(a!) self, int[] size) -> Tensor(a!)", 189},
      {"aten::empty(int[] size, *, Tensor(a!) out) -> Tensor(a!)", 190},
      {"aten::empty_like(Tensor self) -> Tensor", 191},
      {"aten::empty_like(Tensor self, *, ScalarType dtype, Layout layout, Device device, bool pin_memory=False) -> Tensor", 192},
      {"aten::empty_strided(int[] size, int[] stride, *, ScalarType? dtype=None, Layout? layout=None, Device? device=None, bool? pin_memory=None) -> Tensor", 193},
      {"aten::erf(Tensor self) -> Tensor", 194},
      {"aten::erf_(Tensor(a!) self) -> Tensor(a!)", 195},
      {"aten::erf(Tensor self, *, Tensor(a!) out) -> Tensor(a!)", 196},
      {"aten::erfc(Tensor self) -> Tensor", 197},
      {"aten::erfc_(Tensor(a!) self) -> Tensor(a!)", 198},
      {"aten::erfc(Tensor self, *, Tensor(a!) out) -> Tensor(a!)", 199},
      {"aten::exp(Tensor self) -> Tensor", 200},
      {"aten::exp_(Tensor(a!) self) -> Tensor(a!)", 201},
      {"aten::exp(Tensor self, *, Tensor(a!) out) -> Tensor(a!)", 202},
      {"aten::expm1(Tensor self) -> Tensor", 203},
      {"aten::expm1_(Tensor(a!) self) -> Tensor(a!)", 204},
      {"aten::expm1(Tensor self, *, Tensor(a!) out) -> Tensor(a!)", 205},
      {"aten::expand(Tensor(a) self, int[] size, *, bool implicit=False) -> Tensor(a)", 206},
      {"aten::expand_as(Tensor self, Tensor other) -> Tensor", 207},
      {"aten::eye(int n, *, ScalarType? dtype=None, Layout? layout=None, Device? device=None, bool? pin_memory=None) -> Tensor", 208},
      {"aten::eye(int n, int m, *, ScalarType? dtype=None, Layout? layout=None, Device? device=None, bool? pin_memory=None) -> Tensor", 209},
      {"aten::eye(int n, *, Tensor(a!) out) -> Tensor(a!)", 210},
      {"aten::eye(int n, int m, *, Tensor(a!) out) -> Tensor(a!)", 211},
      {"aten::flatten(Tensor self, int start_dim=0, int end_dim=-1) -> Tensor", 212},
      {"aten::fill_(Tensor(a!) self, Scalar value) -> Tensor(a!)", 213},
      {"aten::fill_(Tensor(a!) self, Tensor value) -> Tensor(a!)", 214},
      {"aten::floor(Tensor self) -> Tensor", 215},
      {"aten::floor_(Tensor(a!) self) -> Tensor(a!)", 216},
      {"aten::floor(Tensor self, *, Tensor(a!) out) -> Tensor(a!)", 217},
      {"aten::frac(Tensor self) -> Tensor", 218},
      {"aten::frac_(Tensor(a!) self) -> Tensor(a!)", 219},
      {"aten::frac(Tensor self, *, Tensor(a!) out) -> Tensor(a!)", 220},
      {"aten::full(int[] size, Scalar fill_value, *, ScalarType? dtype=None, Layout? layout=None, Device? device=None, bool? pin_memory=None) -> Tensor", 221},
      {"aten::full(int[] size, Scalar fill_value, *, Tensor(a!) out) -> Tensor(a!)", 222},
      {"aten::full_like(Tensor self, Scalar fill_value) -> Tensor", 223},
      {"aten::full_like(Tensor self, Scalar fill_value, *, ScalarType dtype, Layout layout, Device device, bool pin_memory=False) -> Tensor", 224},
      {"aten::from_file(str filename, bool? shared=None, int? size=0, *, ScalarType? dtype=None, Layout? layout=None, Device? device=None, bool? pin_memory=None) -> Tensor", 225},
      {"aten::grid_sampler(Tensor input, Tensor grid, int interpolation_mode, int padding_mode) -> Tensor", 226},
      {"aten::grid_sampler_2d(Tensor input, Tensor grid, int interpolation_mode, int padding_mode) -> Tensor", 227},
      {"aten::grid_sampler_2d_backward(Tensor grad_output, Tensor input, Tensor grid, int interpolation_mode, int padding_mode) -> (Tensor, Tensor)", 228},
      {"aten::grid_sampler_3d(Tensor input, Tensor grid, int interpolation_mode, int padding_mode) -> Tensor", 229},
      {"aten::grid_sampler_3d_backward(Tensor grad_output, Tensor input, Tensor grid, int interpolation_mode, int padding_mode) -> (Tensor, Tensor)", 230},
      {"aten::hann_window(int window_length, *, ScalarType? dtype=None, Layout? layout=None, Device? device=None, bool? pin_memory=None) -> Tensor", 231},
      {"aten::hann_window(int window_length, bool periodic, *, ScalarType? dtype=None, Layout? layout=None, Device? device=None, bool? pin_memory=None) -> Tensor", 232},
      {"aten::hamming_window(int window_length, *, ScalarType? dtype=None, Layout? layout=None, Device? device=None, bool? pin_memory=None) -> Tensor", 233},
      {"aten::hamming_window(int window_length, bool periodic, *, ScalarType? dtype=None, Layout? layout=None, Device? device=None, bool? pin_memory=None) -> Tensor", 234},
      {"aten::hamming_window(int window_length, bool periodic, float alpha, *, ScalarType? dtype=None, Layout? layout=None, Device? device=None, bool? pin_memory=None) -> Tensor", 235},
      {"aten::hamming_window(int window_length, bool periodic, float alpha, float beta, *, ScalarType? dtype=None, Layout? layout=None, Device? device=None, bool? pin_memory=None) -> Tensor", 236},
      {"aten::hinge_embedding_loss(Tensor self, Tensor target, float margin=1.0, int reduction=Mean) -> Tensor", 237},
      {"aten::ger(Tensor self, Tensor vec2) -> Tensor", 238},
      {"aten::ger(Tensor self, Tensor vec2, *, Tensor(a!) out) -> Tensor(a!)", 239},
      {"aten::group_norm(Tensor input, int num_groups, Tensor? weight=None, Tensor? bias=None, float eps=1e-05, bool cudnn_enabled=True) -> Tensor", 240},
      {"aten::fft(Tensor self, int signal_ndim, bool normalized=False) -> Tensor", 241},
      {"aten::ifft(Tensor self, int signal_ndim, bool normalized=False) -> Tensor", 242},
      {"aten::rfft(Tensor self, int signal_ndim, bool normalized=False, bool onesided=True) -> Tensor", 243},
      {"aten::irfft(Tensor self, int signal_ndim, bool normalized=False, bool onesided=True, int[] signal_sizes=[]) -> Tensor", 244},
      {"aten::_fft_with_size(Tensor self, int signal_ndim, bool complex_input, bool complex_output, bool inverse, int[] checked_signal_sizes, bool normalized, bool onesided, int[] output_sizes) -> Tensor", 245},
      {"aten::_cufft_get_plan_cache_size(int device_index) -> int", 246},
      {"aten::_cufft_get_plan_cache_max_size(int device_index) -> int", 247},
      {"aten::_cufft_set_plan_cache_max_size(int device_index, int max_size) -> void", 248},
      {"aten::_cufft_clear_plan_cache(int device_index) -> void", 249},
      {"aten::index(Tensor self, Tensor?[] indices) -> Tensor", 250},
      {"aten::index_copy_(Tensor(a!) self, int dim, Tensor index, Tensor source) -> Tensor(a!)", 251},
      {"aten::index_copy(Tensor self, int dim, Tensor index, Tensor source) -> Tensor", 252},
      {"aten::index_put_(Tensor(a!) self, Tensor?[] indices, Tensor values, bool accumulate=False) -> Tensor(a!)", 253},
      {"aten::index_put(Tensor self, Tensor?[] indices, Tensor values, bool accumulate=False) -> Tensor", 254},
      {"aten::instance_norm(Tensor input, Tensor? weight, Tensor? bias, Tensor? running_mean, Tensor? running_var, bool use_input_stats, float momentum, float eps, bool cudnn_enabled) -> Tensor", 255},
      {"aten::inverse(Tensor self) -> Tensor", 256},
      {"aten::inverse(Tensor self, *, Tensor(a!) out) -> Tensor(a!)", 257},
      {"aten::_inverse_helper(Tensor self) -> Tensor", 258},
      {"aten::isclose(Tensor self, Tensor other, float rtol=1e-05, float atol=1e-08, bool equal_nan=False) -> Tensor", 259},
      {"aten::isnan(Tensor self) -> Tensor", 260},
      {"aten::is_distributed(Tensor self) -> bool", 261},
      {"aten::is_floating_point(Tensor self) -> bool", 262},
      {"aten::is_complex(Tensor self) -> bool", 263},
      {"aten::is_nonzero(Tensor self) -> bool", 264},
      {"aten::is_same_size(Tensor self, Tensor other) -> bool", 265},
      {"aten::is_signed(Tensor self) -> bool", 266},
      {"aten::kl_div(Tensor self, Tensor target, int reduction=Mean) -> Tensor", 267},
      {"aten::kl_div_backward(Tensor grad_output, Tensor self, Tensor target, int reduction=Mean) -> Tensor", 268},
      {"aten::kthvalue(Tensor self, int k, int dim=-1, bool keepdim=False) -> (Tensor values, Tensor indices)", 269},
      {"aten::kthvalue(Tensor self, int k, int dim=-1, bool keepdim=False, *, Tensor(a!) values, Tensor(b!) indices) -> (Tensor(a!) values, Tensor(b!) indices)", 270},
      {"aten::layer_norm(Tensor input, int[] normalized_shape, Tensor? weight=None, Tensor? bias=None, float eps=1e-05, bool cudnn_enable=True) -> Tensor", 271},
      {"aten::native_layer_norm(Tensor input, Tensor? weight, Tensor? bias, int M, int N, float eps) -> (Tensor, Tensor, Tensor)", 272},
      {"aten::native_layer_norm_backward(Tensor grad_out, Tensor input, Tensor mean, Tensor rstd, Tensor? weight, int M, int N, bool[3] output_mask) -> (Tensor, Tensor, Tensor)", 273},
      {"aten::native_layer_norm_double_backward(Tensor? ggI, Tensor? ggW, Tensor? ggb, Tensor gO, Tensor input, Tensor mean, Tensor rstd, Tensor? weight, int M, int N, bool[3] output_mask) -> (Tensor, Tensor, Tensor)", 274},
      {"aten::linear(Tensor input, Tensor weight, Tensor? bias=None) -> Tensor", 275},
      {"aten::mkldnn_linear(Tensor input, Tensor weight, Tensor? bias=None) -> Tensor", 276},
      {"aten::fbgemm_linear_int8_weight(Tensor input, Tensor weight, Tensor packed, Tensor col_offsets, Scalar weight_scale, Scalar weight_zero_point, Tensor bias) -> Tensor", 277},
      {"aten::fbgemm_linear_quantize_weight(Tensor input) -> (Tensor, Tensor, float, int)", 278},
      {"aten::fbgemm_pack_quantized_matrix(Tensor input, int K, int N) -> Tensor", 279},
      {"aten::fbgemm_is_cpu_supported() -> bool", 280},
      {"aten::linspace(Scalar start, Scalar end, int steps=100, *, ScalarType? dtype=None, Layout? layout=None, Device? device=None, bool? pin_memory=None) -> Tensor", 281},
      {"aten::linspace(Scalar start, Scalar end, int steps=100, *, Tensor(a!) out) -> Tensor(a!)", 282},
      {"aten::log(Tensor self) -> Tensor", 283},
      {"aten::log_(Tensor(a!) self) -> Tensor(a!)", 284},
      {"aten::log(Tensor self, *, Tensor(a!) out) -> Tensor(a!)", 285},
      {"aten::log10(Tensor self) -> Tensor", 286},
      {"aten::log10_(Tensor(a!) self) -> Tensor(a!)", 287},
      {"aten::log10(Tensor self, *, Tensor(a!) out) -> Tensor(a!)", 288},
      {"aten::log1p(Tensor self) -> Tensor", 289},
      {"aten::log1p_(Tensor(a!) self) -> Tensor(a!)", 290},
      {"aten::log1p(Tensor self, *, Tensor(a!) out) -> Tensor(a!)", 291},
      {"aten::log2(Tensor self) -> Tensor", 292},
      {"aten::log2_(Tensor(a!) self) -> Tensor(a!)", 293},
      {"aten::log2(Tensor self, *, Tensor(a!) out) -> Tensor(a!)", 294},
      {"aten::logdet(Tensor self) -> Tensor", 295},
      {"aten::logspace(Scalar start, Scalar end, int steps=100, float base=10.0, *, ScalarType? dtype=None, Layout? layout=None, Device? device=None, bool? pin_memory=None) -> Tensor", 296},
      {"aten::logspace(Scalar start, Scalar end, int steps=100, float base=10.0, *, Tensor(a!) out) -> Tensor(a!)", 297},
      {"aten::log_softmax(Tensor self, int dim, ScalarType dtype) -> Tensor", 298},
      {"aten::log_softmax(Tensor self, int dim) -> Tensor", 299},
      {"aten::_log_softmax(Tensor self, int dim, bool half_to_float) -> Tensor", 300},
      {"aten::_log_softmax_backward_data(Tensor grad_output, Tensor output, int dim, Tensor self) -> Tensor", 301},
      {"aten::logsumexp(Tensor self, int[1] dim, bool keepdim=False) -> Tensor", 302},
      {"aten::logsumexp(Tensor self, int[1] dim, bool keepdim=False, *, Tensor(a!) out) -> Tensor(a!)", 303},
      {"aten::margin_ranking_loss(Tensor input1, Tensor input2, Tensor target, float margin=0.0, int reduction=Mean) -> Tensor", 304},
      {"aten::matmul(Tensor self, Tensor other) -> Tensor", 305},
      {"aten::matmul(Tensor self, Tensor other, *, Tensor(a!) out) -> Tensor(a!)", 306},
      {"aten::matrix_rank(Tensor self, float tol, bool symmetric=False) -> Tensor", 307},
      {"aten::matrix_rank(Tensor self, bool symmetric=False) -> Tensor", 308},
      {"aten::matrix_power(Tensor self, int n) -> Tensor", 309},
      {"aten::max(Tensor self, int dim, bool keepdim=False) -> (Tensor values, Tensor indices)", 310},
      {"aten::max(Tensor self, int dim, bool keepdim=False, *, Tensor(a!) max, Tensor(b!) max_values) -> (Tensor(a!) values, Tensor(b!) indices)", 311},
      {"aten::max_values(Tensor self, int[1] dim, bool keepdim=False) -> Tensor", 312},
      {"aten::max_pool1d_with_indices(Tensor self, int[1] kernel_size, int[1] stride=[], int[1] padding=0, int[1] dilation=1, bool ceil_mode=False) -> (Tensor, Tensor)", 313},
      {"aten::max_pool1d(Tensor self, int[1] kernel_size, int[1] stride=[], int[1] padding=0, int[1] dilation=1, bool ceil_mode=False) -> Tensor", 314},
      {"aten::max_pool2d(Tensor self, int[2] kernel_size, int[2] stride=[], int[2] padding=0, int[2] dilation=1, bool ceil_mode=False) -> Tensor", 315},
      {"aten::mkldnn_max_pool2d(Tensor self, int[2] kernel_size, int[2] stride=[], int[2] padding=0, int[2] dilation=1, bool ceil_mode=False) -> Tensor", 316},
      {"aten::max_pool3d(Tensor self, int[3] kernel_size, int[3] stride=[], int[3] padding=0, int[3] dilation=1, bool ceil_mode=False) -> Tensor", 317},
      {"aten::mean(Tensor self, *, ScalarType dtype) -> Tensor", 318},
      {"aten::mean(Tensor self) -> Tensor", 319},
      {"aten::mean(Tensor self, int[1] dim, bool keepdim, *, ScalarType dtype) -> Tensor", 320},
      {"aten::mean(Tensor self, int[1] dim, bool keepdim=False) -> Tensor", 321},
      {"aten::mean(Tensor self, int[1] dim, *, ScalarType dtype) -> Tensor", 322},
      {"aten::mean(Tensor self, int[1] dim, bool keepdim, *, ScalarType dtype, Tensor(a!) out) -> Tensor(a!)", 323},
      {"aten::mean(Tensor self, int[1] dim, bool keepdim=False, *, Tensor(a!) out) -> Tensor(a!)", 324},
      {"aten::mean(Tensor self, int[1] dim, *, ScalarType dtype, Tensor(a!) out) -> Tensor(a!)", 325},
      {"aten::median(Tensor self, int dim, bool keepdim=False) -> (Tensor values, Tensor indices)", 326},
      {"aten::median(Tensor self, int dim, bool keepdim=False, *, Tensor(a!) values, Tensor(b!) indices) -> (Tensor(a!) values, Tensor(b!) indices)", 327},
      {"aten::min(Tensor self, int dim, bool keepdim=False) -> (Tensor values, Tensor indices)", 328},
      {"aten::min(Tensor self, int dim, bool keepdim=False, *, Tensor(a!) min, Tensor(b!) min_indices) -> (Tensor(a!) values, Tensor(b!) indices)", 329},
      {"aten::min_values(Tensor self, int[1] dim, bool keepdim=False) -> Tensor", 330},
      {"aten::mkldnn_convolution(Tensor self, Tensor weight, Tensor? bias, int[] padding, int[] stride, int[] dilation, int groups) -> Tensor", 331},
      {"aten::mkldnn_convolution_backward_input(int[] self_size, Tensor grad_output, Tensor weight, int[] padding, int[] stride, int[] dilation, int groups, bool bias_defined) -> Tensor", 332},
      {"aten::mkldnn_convolution_backward_weights(int[] weight_size, Tensor grad_output, Tensor self, int[] padding, int[] stride, int[] dilation, int groups, bool bias_defined) -> (Tensor, Tensor)", 333},
      {"aten::mkldnn_convolution_backward(Tensor self, Tensor grad_output, Tensor weight, int[] padding, int[] stride, int[] dilation, int groups, bool[3] output_mask) -> (Tensor, Tensor, Tensor)", 334},
      {"aten::miopen_batch_norm(Tensor input, Tensor weight, Tensor? bias, Tensor? running_mean, Tensor? running_var, bool training, float exponential_average_factor, float epsilon) -> (Tensor, Tensor, Tensor)", 335},
      {"aten::miopen_batch_norm_backward(Tensor input, Tensor grad_output, Tensor weight, Tensor? running_mean, Tensor? running_var, Tensor? save_mean, Tensor? save_var, float epsilon) -> (Tensor, Tensor, Tensor)", 336},
      {"aten::miopen_convolution(Tensor self, Tensor weight, Tensor? bias, int[] padding, int[] stride, int[] dilation, int groups, bool benchmark, bool deterministic) -> Tensor", 337},
      {"aten::miopen_convolution_backward_input(int[] self_size, Tensor grad_output, Tensor weight, int[] padding, int[] stride, int[] dilation, int groups, bool benchmark, bool deterministic) -> Tensor", 338},
      {"aten::miopen_convolution_backward(Tensor self, Tensor grad_output, Tensor weight, int[] padding, int[] stride, int[] dilation, int groups, bool benchmark, bool deterministic, bool[3] output_mask) -> (Tensor, Tensor, Tensor)", 339},
      {"aten::miopen_convolution_backward_bias(Tensor grad_output) -> Tensor", 340},
      {"aten::miopen_convolution_backward_weight(int[] weight_size, Tensor grad_output, Tensor self, int[] padding, int[] stride, int[] dilation, int groups, bool benchmark, bool deterministic) -> Tensor", 341},
      {"aten::miopen_convolution_transpose(Tensor self, Tensor weight, Tensor? bias, int[] padding, int[] output_padding, int[] stride, int[] dilation, int groups, bool benchmark, bool deterministic) -> Tensor", 342},
      {"aten::miopen_convolution_transpose_backward(Tensor self, Tensor grad_output, Tensor weight, int[] padding, int[] output_padding, int[] stride, int[] dilation, int groups, bool benchmark, bool deterministic, bool[3] output_mask) -> (Tensor, Tensor, Tensor)", 343},
      {"aten::miopen_convolution_transpose_backward_input(Tensor grad_output, Tensor weight, int[] padding, int[] stride, int[] dilation, int groups, bool benchmark, bool deterministic) -> Tensor", 344},
      {"aten::miopen_convolution_transpose_backward_weight(int[] weight_size, Tensor grad_output, Tensor self, int[] padding, int[] stride, int[] dilation, int groups, bool benchmark, bool deterministic) -> Tensor", 345},
      {"aten::miopen_depthwise_convolution(Tensor self, Tensor weight, Tensor? bias, int[] padding, int[] stride, int[] dilation, int groups, bool benchmark, bool deterministic) -> Tensor", 346},
      {"aten::miopen_depthwise_convolution_backward_input(int[] self_size, Tensor grad_output, Tensor weight, int[] padding, int[] stride, int[] dilation, int groups, bool benchmark, bool deterministic) -> Tensor", 347},
      {"aten::miopen_depthwise_convolution_backward(Tensor self, Tensor grad_output, Tensor weight, int[] padding, int[] stride, int[] dilation, int groups, bool benchmark, bool deterministic, bool[3] output_mask) -> (Tensor, Tensor, Tensor)", 348},
      {"aten::miopen_depthwise_convolution_backward_weight(int[] weight_size, Tensor grad_output, Tensor self, int[] padding, int[] stride, int[] dilation, int groups, bool benchmark, bool deterministic) -> Tensor", 349},
      {"aten::mm(Tensor self, Tensor mat2) -> Tensor", 350},
      {"aten::mm(Tensor self, Tensor mat2, *, Tensor(a!) out) -> Tensor(a!)", 351},
      {"aten::_sparse_mm(Tensor sparse, Tensor dense) -> Tensor", 352},
      {"aten::mode(Tensor self, int dim=-1, bool keepdim=False) -> (Tensor values, Tensor indices)", 353},
      {"aten::mode(Tensor self, int dim=-1, bool keepdim=False, *, Tensor(a!) values, Tensor(b!) indices) -> (Tensor(a!) values, Tensor(b!) indices)", 354},
      {"aten::mul(Tensor self, Tensor other) -> Tensor", 355},
      {"aten::mul_(Tensor(a!) self, Tensor other) -> Tensor(a!)", 356},
      {"aten::mul(Tensor self, Tensor other, *, Tensor(a!) out) -> Tensor(a!)", 357},
      {"aten::mul(Tensor self, Scalar other) -> Tensor", 358},
      {"aten::mul_(Tensor(a!) self, Scalar other) -> Tensor(a!)", 359},
      {"aten::mv(Tensor self, Tensor vec) -> Tensor", 360},
      {"aten::mv(Tensor self, Tensor vec, *, Tensor(a!) out) -> Tensor(a!)", 361},
      {"aten::mvlgamma(Tensor self, int p) -> Tensor", 362},
      {"aten::mvlgamma_(Tensor(a!) self, int p) -> Tensor(a!)", 363},
      {"aten::narrow_copy(Tensor self, int dim, int start, int length) -> Tensor", 364},
      {"aten::narrow(Tensor(a) self, int dim, int start, int length) -> Tensor(a)", 365},
      {"aten::native_batch_norm(Tensor input, Tensor? weight, Tensor? bias, Tensor? running_mean, Tensor? running_var, bool training, float momentum, float eps) -> (Tensor, Tensor, Tensor)", 366},
      {"aten::batch_norm_stats(Tensor input, float eps) -> (Tensor, Tensor)", 367},
      {"aten::batch_norm_elemt(Tensor input, Tensor? weight, Tensor? bias, Tensor mean, Tensor invstd, float eps) -> Tensor", 368},
      {"aten::batch_norm_gather_stats(Tensor input, Tensor mean, Tensor invstd, Tensor? running_mean, Tensor? running_var, float momentum, float eps, int count) -> (Tensor, Tensor)", 369},
      {"aten::native_batch_norm_backward(Tensor grad_out, Tensor input, Tensor? weight, Tensor? running_mean, Tensor? running_var, Tensor? save_mean, Tensor? save_invstd, bool train, float eps, bool[3] output_mask) -> (Tensor, Tensor, Tensor)", 370},
      {"aten::batch_norm_backward_reduce(Tensor grad_out, Tensor input, Tensor mean, Tensor invstd, bool input_g, bool weight_g, bool bias_g) -> (Tensor, Tensor, Tensor, Tensor)", 371},
      {"aten::batch_norm_backward_elemt(Tensor grad_out, Tensor input, Tensor mean, Tensor invstd, Tensor? weight, Tensor mean_dy, Tensor mean_dy_xmu) -> Tensor", 372},
      {"aten::batch_norm_update_stats(Tensor input, Tensor? running_mean, Tensor? running_var, float momentum) -> (Tensor, Tensor)", 373},
      {"aten::_nnpack_available() -> bool", 374},
      {"aten::_nnpack_spatial_convolution(Tensor input, Tensor weight, Tensor? bias, int[2] padding) -> Tensor", 375},
      {"aten::_nnpack_spatial_convolution_backward(Tensor input, Tensor grad_output, Tensor weight, int[2] padding, bool[3] output_mask) -> (Tensor, Tensor, Tensor)", 376},
      {"aten::_nnpack_spatial_convolution_backward_input(Tensor input, Tensor grad_output, Tensor weight, int[2] padding) -> Tensor", 377},
      {"aten::_nnpack_spatial_convolution_backward_weight(Tensor input, int[] weightsize, Tensor grad_output, int[2] padding) -> Tensor", 378},
      {"aten::ones(int[] size, *, ScalarType? dtype=None, Layout? layout=None, Device? device=None, bool? pin_memory=None) -> Tensor", 379},
      {"aten::ones(int[] size, *, Tensor(a!) out) -> Tensor(a!)", 380},
      {"aten::ones_like(Tensor self) -> Tensor", 381},
      {"aten::ones_like(Tensor self, *, ScalarType dtype, Layout layout, Device device, bool pin_memory=False) -> Tensor", 382},
      {"aten::pairwise_distance(Tensor x1, Tensor x2, float p=2, float eps=1e-06, bool keepdim=False) -> Tensor", 383},
      {"aten::cdist(Tensor x1, Tensor x2, float p=2) -> Tensor", 384},
      {"aten::_cdist_backward(Tensor grad, Tensor x1, Tensor x2, float p, Tensor cdist) -> Tensor", 385},
      {"aten::pdist(Tensor self, float p=2) -> Tensor", 386},
      {"aten::_pdist_forward(Tensor self, float p=2) -> Tensor", 387},
      {"aten::_pdist_backward(Tensor grad, Tensor self, float p, Tensor pdist) -> Tensor", 388},
      {"aten::cosine_similarity(Tensor x1, Tensor x2, int dim=1, float eps=1e-08) -> Tensor", 389},
      {"aten::permute(Tensor(a) self, int[] dims) -> Tensor(a)", 390},
      {"aten::pixel_shuffle(Tensor self, int upscale_factor) -> Tensor", 391},
      {"aten::pin_memory(Tensor self) -> Tensor", 392},
      {"aten::pinverse(Tensor self, float rcond=1e-15) -> Tensor", 393},
      {"aten::poisson_nll_loss(Tensor input, Tensor target, bool log_input, bool full, float eps, int reduction) -> Tensor", 394},
      {"aten::scalar_tensor(Scalar s, *, ScalarType? dtype=None, Layout? layout=None, Device? device=None, bool? pin_memory=None) -> Tensor", 395},
      {"aten::rand(int[] size, *, ScalarType? dtype=None, Layout? layout=None, Device? device=None, bool? pin_memory=None) -> Tensor", 396},
      {"aten::rand(int[] size, *, Generator? generator, ScalarType? dtype=None, Layout? layout=None, Device? device=None, bool? pin_memory=None) -> Tensor", 397},
      {"aten::rand(int[] size, *, Tensor(a!) out) -> Tensor(a!)", 398},
      {"aten::rand(int[] size, *, Generator? generator, Tensor(a!) out) -> Tensor(a!)", 399},
      {"aten::rand_like(Tensor self) -> Tensor", 400},
      {"aten::rand_like(Tensor self, *, ScalarType dtype, Layout layout, Device device, bool pin_memory=False) -> Tensor", 401},
      {"aten::randint(int high, int[] size, *, ScalarType? dtype=None, Layout? layout=None, Device? device=None, bool? pin_memory=None) -> Tensor", 402},
      {"aten::randint(int high, int[] size, *, Generator? generator, ScalarType? dtype=None, Layout? layout=None, Device? device=None, bool? pin_memory=None) -> Tensor", 403},
      {"aten::randint(int low, int high, int[] size, *, ScalarType? dtype=None, Layout? layout=None, Device? device=None, bool? pin_memory=None) -> Tensor", 404},
      {"aten::randint(int low, int high, int[] size, *, Generator? generator, ScalarType? dtype=None, Layout? layout=None, Device? device=None, bool? pin_memory=None) -> Tensor", 405},
      {"aten::randint(int high, int[] size, *, Tensor(a!) out) -> Tensor(a!)", 406},
      {"aten::randint(int high, int[] size, *, Generator? generator, Tensor(a!) out) -> Tensor(a!)", 407},
      {"aten::randint(int low, int high, int[] size, *, Tensor(a!) out) -> Tensor(a!)", 408},
      {"aten::randint(int low, int high, int[] size, *, Generator? generator, Tensor(a!) out) -> Tensor(a!)", 409},
      {"aten::randint_like(Tensor self, int high) -> Tensor", 410},
      {"aten::randint_like(Tensor self, int low, int high) -> Tensor", 411},
      {"aten::randint_like(Tensor self, int high, *, ScalarType dtype, Layout layout, Device device, bool pin_memory=False) -> Tensor", 412},
      {"aten::randint_like(Tensor self, int low, int high, *, ScalarType dtype, Layout layout, Device device, bool pin_memory=False) -> Tensor", 413},
      {"aten::randn(int[] size, *, ScalarType? dtype=None, Layout? layout=None, Device? device=None, bool? pin_memory=None) -> Tensor", 414},
      {"aten::randn(int[] size, *, Generator? generator, ScalarType? dtype=None, Layout? layout=None, Device? device=None, bool? pin_memory=None) -> Tensor", 415},
      {"aten::randn(int[] size, *, Tensor(a!) out) -> Tensor(a!)", 416},
      {"aten::randn(int[] size, *, Generator? generator, Tensor(a!) out) -> Tensor(a!)", 417},
      {"aten::randn_like(Tensor self) -> Tensor", 418},
      {"aten::randn_like(Tensor self, *, ScalarType dtype, Layout layout, Device device, bool pin_memory=False) -> Tensor", 419},
      {"aten::randperm(int n, *, ScalarType? dtype=None, Layout? layout=None, Device? device=None, bool? pin_memory=None) -> Tensor", 420},
      {"aten::randperm(int n, *, Generator? generator, ScalarType? dtype=None, Layout? layout=None, Device? device=None, bool? pin_memory=None) -> Tensor", 421},
      {"aten::randperm(int n, *, Tensor(a!) out) -> Tensor(a!)", 422},
      {"aten::randperm(int n, *, Generator? generator, Tensor(a!) out) -> Tensor(a!)", 423},
      {"aten::range(Scalar start, Scalar end, Scalar step=1, *, ScalarType? dtype=None, Layout? layout=None, Device? device=None, bool? pin_memory=None) -> Tensor", 424},
      {"aten::range(Scalar start, Scalar end, *, ScalarType? dtype=None, Layout? layout=None, Device? device=None, bool? pin_memory=None) -> Tensor", 425},
      {"aten::range(Scalar start, Scalar end, Scalar step=1, *, Tensor(a!) out) -> Tensor(a!)", 426},
      {"aten::reciprocal(Tensor self) -> Tensor", 427},
      {"aten::reciprocal_(Tensor(a!) self) -> Tensor(a!)", 428},
      {"aten::reciprocal(Tensor self, *, Tensor(a!) out) -> Tensor(a!)", 429},
      {"aten::neg(Tensor self) -> Tensor", 430},
      {"aten::neg_(Tensor(a!) self) -> Tensor(a!)", 431},
      {"aten::neg(Tensor self, *, Tensor(a!) out) -> Tensor(a!)", 432},
      {"aten::repeat(Tensor self, int[] repeats) -> Tensor", 433},
      {"aten::repeat_interleave(Tensor repeats) -> Tensor", 434},
      {"aten::repeat_interleave(Tensor self, Tensor repeats, int? dim=None) -> Tensor", 435},
      {"aten::repeat_interleave(Tensor self, int repeats, int? dim=None) -> Tensor", 436},
      {"aten::reshape(Tensor self, int[] shape) -> Tensor", 437},
      {"aten::mkldnn_reshape(Tensor self, int[] shape) -> Tensor", 438},
      {"aten::reshape_as(Tensor self, Tensor other) -> Tensor", 439},
      {"aten::round(Tensor self) -> Tensor", 440},
      {"aten::round_(Tensor(a!) self) -> Tensor(a!)", 441},
      {"aten::round(Tensor self, *, Tensor(a!) out) -> Tensor(a!)", 442},
      {"aten::rrelu(Tensor self, Scalar lower=0.125, Scalar upper=0.3333333333333333, bool training=False, Generator? generator=None) -> Tensor", 443},
      {"aten::rrelu_(Tensor(a!) self, Scalar lower=0.125, Scalar upper=0.3333333333333333, bool training=False, Generator? generator=None) -> Tensor(a!)", 444},
      {"aten::relu(Tensor self) -> Tensor", 445},
      {"aten::relu_(Tensor(a!) self) -> Tensor(a!)", 446},
      {"aten::prelu(Tensor self, Tensor weight) -> Tensor", 447},
      {"aten::prelu_backward(Tensor grad_output, Tensor self, Tensor weight) -> (Tensor, Tensor)", 448},
      {"aten::gelu(Tensor self) -> Tensor", 449},
      {"aten::gelu_backward(Tensor grad, Tensor self) -> Tensor", 450},
      {"aten::hardshrink(Tensor self, Scalar lambd=0.5) -> Tensor", 451},
      {"aten::hardshrink_backward(Tensor grad_out, Tensor self, Scalar lambd) -> Tensor", 452},
      {"aten::rsqrt(Tensor self) -> Tensor", 453},
      {"aten::rsqrt_(Tensor(a!) self) -> Tensor(a!)", 454},
      {"aten::rsqrt(Tensor self, *, Tensor(a!) out) -> Tensor(a!)", 455},
      {"aten::select(Tensor(a) self, int dim, int index) -> Tensor(a)", 456},
      {"aten::selu(Tensor self) -> Tensor", 457},
      {"aten::selu_(Tensor(a!) self) -> Tensor(a!)", 458},
      {"aten::celu(Tensor self, Scalar alpha=1.0) -> Tensor", 459},
      {"aten::celu_(Tensor(a!) self, Scalar alpha=1.0) -> Tensor(a!)", 460},
      {"aten::sigmoid(Tensor self) -> Tensor", 461},
      {"aten::sigmoid_(Tensor(a!) self) -> Tensor(a!)", 462},
      {"aten::sigmoid(Tensor self, *, Tensor(a!) out) -> Tensor(a!)", 463},
      {"aten::sin(Tensor self) -> Tensor", 464},
      {"aten::sin_(Tensor(a!) self) -> Tensor(a!)", 465},
      {"aten::sin(Tensor self, *, Tensor(a!) out) -> Tensor(a!)", 466},
      {"aten::sinh(Tensor self) -> Tensor", 467},
      {"aten::sinh_(Tensor(a!) self) -> Tensor(a!)", 468},
      {"aten::sinh(Tensor self, *, Tensor(a!) out) -> Tensor(a!)", 469},
      {"aten::detach(Tensor self) -> Tensor", 470},
      {"aten::detach_(Tensor(a!) self) -> Tensor(a!)", 471},
      {"aten::size(Tensor self, int dim) -> int", 472},
      {"aten::slice(Tensor(a) self, int dim=0, int start=0, int end=9223372036854775807, int step=1) -> Tensor(a)", 473},
      {"aten::slogdet(Tensor self) -> (Tensor sign, Tensor logabsdet)", 474},
      {"aten::smm(Tensor self, Tensor mat2) -> Tensor", 475},
      {"aten::softmax(Tensor self, int dim, ScalarType dtype) -> Tensor", 476},
      {"aten::softmax(Tensor self, int dim) -> Tensor", 477},
      {"aten::_softmax(Tensor self, int dim, bool half_to_float) -> Tensor", 478},
      {"aten::_softmax_backward_data(Tensor grad_output, Tensor output, int dim, Tensor self) -> Tensor", 479},
      {"aten::_sparse_add(Tensor self, Tensor other, *, Scalar alpha=1, Tensor(a!) out) -> Tensor(a!)", 480},
      {"aten::_sparse_dense_add(Tensor self, Tensor other, *, Scalar alpha=1, Tensor(a!) out) -> Tensor(a!)", 481},
      {"aten::_sparse_div_zerodim(Tensor self, Tensor other, *, Tensor(a!) out) -> Tensor(a!)", 482},
      {"aten::_sparse_div_scalar(Tensor self, Scalar other, *, Tensor(a!) out) -> Tensor(a!)", 483},
      {"aten::_sparse_mul(Tensor self, Tensor other, *, Tensor(a!) out) -> Tensor(a!)", 484},
      {"aten::_sparse_mul_zerodim(Tensor self, Tensor other, *, Tensor(a!) out) -> Tensor(a!)", 485},
      {"aten::_sparse_mul_scalar(Tensor self, Scalar other, *, Tensor(a!) out) -> Tensor(a!)", 486},
      {"aten::split(Tensor(a) self, int split_size, int dim=0) -> Tensor(a)[]", 487},
      {"aten::split_with_sizes(Tensor self, int[] split_sizes, int dim=0) -> Tensor[]", 488},
      {"aten::squeeze(Tensor(a) self) -> Tensor(a)", 489},
      {"aten::squeeze(Tensor(a) self, int dim) -> Tensor(a)", 490},
      {"aten::squeeze_(Tensor(a!) self) -> Tensor(a!)", 491},
      {"aten::squeeze_(Tensor(a!) self, int dim) -> Tensor(a!)", 492},
      {"aten::sspaddmm(Tensor self, Tensor mat1, Tensor mat2, *, Scalar beta=1, Scalar alpha=1) -> Tensor", 493},
      {"aten::sspaddmm(Tensor self, Tensor mat1, Tensor mat2, *, Scalar beta=1, Scalar alpha=1, Tensor(a!) out) -> Tensor(a!)", 494},
      {"aten::stack(Tensor[] tensors, int dim=0) -> Tensor", 495},
      {"aten::stack(Tensor[] tensors, int dim=0, *, Tensor(a!) out) -> Tensor(a!)", 496},
      {"aten::stft(Tensor self, int n_fft, int? hop_length=None, int? win_length=None, Tensor? window=None, bool normalized=False, bool onesided=True) -> Tensor", 497},
      {"aten::stride(Tensor self, int dim) -> int", 498},
      {"aten::sum(Tensor self, *, ScalarType dtype) -> Tensor", 499},
      {"aten::sum(Tensor self) -> Tensor", 500},
      {"aten::sum(Tensor self, int[1] dim, bool keepdim, *, ScalarType dtype) -> Tensor", 501},
      {"aten::sum(Tensor self, int[1] dim, bool keepdim=False) -> Tensor", 502},
      {"aten::sum(Tensor self, int[1] dim, *, ScalarType dtype) -> Tensor", 503},
      {"aten::sum(Tensor self, int[1] dim, bool keepdim, *, ScalarType dtype, Tensor(a!) out) -> Tensor(a!)", 504},
      {"aten::sum(Tensor self, int[1] dim, bool keepdim=False, *, Tensor(a!) out) -> Tensor(a!)", 505},
      {"aten::sum(Tensor self, int[1] dim, *, ScalarType dtype, Tensor(a!) out) -> Tensor(a!)", 506},
      {"aten::sum_to_size(Tensor self, int[] size) -> Tensor", 507},
      {"aten::sqrt(Tensor self) -> Tensor", 508},
      {"aten::sqrt_(Tensor(a!) self) -> Tensor(a!)", 509},
      {"aten::sqrt(Tensor self, *, Tensor(a!) out) -> Tensor(a!)", 510},
      {"aten::std(Tensor self, bool unbiased=True) -> Tensor", 511},
      {"aten::std(Tensor self, int[1] dim, bool unbiased=True, bool keepdim=False) -> Tensor", 512},
      {"aten::std_mean(Tensor self, bool unbiased=True) -> (Tensor, Tensor)", 513},
      {"aten::std_mean(Tensor self, int[1] dim, bool unbiased=True, bool keepdim=False) -> (Tensor, Tensor)", 514},
      {"aten::std(Tensor self, int[1] dim, bool unbiased=True, bool keepdim=False, *, Tensor(a!) out) -> Tensor(a!)", 515},
      {"aten::prod(Tensor self, *, ScalarType dtype) -> Tensor", 516},
      {"aten::prod(Tensor self) -> Tensor", 517},
      {"aten::prod(Tensor self, int dim, bool keepdim, *, ScalarType dtype) -> Tensor", 518},
      {"aten::prod(Tensor self, int dim, bool keepdim=False) -> Tensor", 519},
      {"aten::prod(Tensor self, int dim, *, ScalarType dtype) -> Tensor", 520},
      {"aten::prod(Tensor self, int dim, bool keepdim, *, ScalarType dtype, Tensor(a!) out) -> Tensor(a!)", 521},
      {"aten::prod(Tensor self, int dim, bool keepdim=False, *, Tensor(a!) out) -> Tensor(a!)", 522},
      {"aten::prod(Tensor self, int dim, *, ScalarType dtype, Tensor(a!) out) -> Tensor(a!)", 523},
      {"aten::t(Tensor(a) self) -> Tensor(a)", 524},
      {"aten::t_(Tensor(a!) self) -> Tensor(a!)", 525},
      {"aten::tan(Tensor self) -> Tensor", 526},
      {"aten::tan_(Tensor(a!) self) -> Tensor(a!)", 527},
      {"aten::tan(Tensor self, *, Tensor(a!) out) -> Tensor(a!)", 528},
      {"aten::tanh(Tensor self) -> Tensor", 529},
      {"aten::tanh_(Tensor(a!) self) -> Tensor(a!)", 530},
      {"aten::tanh(Tensor self, *, Tensor(a!) out) -> Tensor(a!)", 531},
      {"aten::tensordot(Tensor self, Tensor other, int[] dims_self, int[] dims_other) -> Tensor", 532},
      {"aten::threshold(Tensor self, Scalar threshold, Scalar value) -> Tensor", 533},
      {"aten::threshold_(Tensor(a!) self, Scalar threshold, Scalar value) -> Tensor(a!)", 534},
      {"aten::threshold(Tensor self, Scalar threshold, Scalar value, *, Tensor(a!) out) -> Tensor(a!)", 535},
      {"aten::threshold_backward(Tensor grad_output, Tensor self, Scalar threshold) -> Tensor", 536},
      {"aten::transpose(Tensor(a) self, int dim0, int dim1) -> Tensor(a)", 537},
      {"aten::transpose_(Tensor(a!) self, int dim0, int dim1) -> Tensor(a!)", 538},
      {"aten::one_hot(Tensor self, int num_classes=-1) -> Tensor", 539},
      {"aten::flip(Tensor self, int[] dims) -> Tensor", 540},
      {"aten::roll(Tensor self, int[1] shifts, int[1] dims=[]) -> Tensor", 541},
      {"aten::rot90(Tensor self, int k=1, int[] dims=[0,1]) -> Tensor", 542},
      {"aten::_trilinear(Tensor i1, Tensor i2, Tensor i3, int[] expand1, int[] expand2, int[] expand3, int[] sumdim, int unroll_dim=1) -> Tensor", 543},
      {"aten::triplet_margin_loss(Tensor anchor, Tensor positive, Tensor negative, float margin=1.0, float p=2, float eps=1e-06, bool swap=False, int reduction=Mean) -> Tensor", 544},
      {"aten::trunc(Tensor self) -> Tensor", 545},
      {"aten::trunc_(Tensor(a!) self) -> Tensor(a!)", 546},
      {"aten::trunc(Tensor self, *, Tensor(a!) out) -> Tensor(a!)", 547},
      {"aten::type_as(Tensor self, Tensor other) -> Tensor", 548},
      {"aten::_unique(Tensor self, bool sorted=True, bool return_inverse=False) -> (Tensor, Tensor)", 549},
      {"aten::unique_dim(Tensor self, int dim, bool sorted=True, bool return_inverse=False, bool return_counts=False) -> (Tensor, Tensor, Tensor)", 550},
      {"aten::unique_consecutive(Tensor self, bool return_inverse=False, bool return_counts=False, int? dim=None) -> (Tensor, Tensor, Tensor)", 551},
      {"aten::unique_dim_consecutive(Tensor self, int dim, bool return_inverse=False, bool return_counts=False) -> (Tensor, Tensor, Tensor)", 552},
      {"aten::_unique2(Tensor self, bool sorted=True, bool return_inverse=False, bool return_counts=False) -> (Tensor, Tensor, Tensor)", 553},
      {"aten::_unsafe_view(Tensor self, int[] size) -> Tensor", 554},
      {"aten::unsqueeze(Tensor(a) self, int dim) -> Tensor(a)", 555},
      {"aten::unsqueeze_(Tensor(a!) self, int dim) -> Tensor(a!)", 556},
      {"aten::var(Tensor self, bool unbiased=True) -> Tensor", 557},
      {"aten::var(Tensor self, int[1] dim, bool unbiased=True, bool keepdim=False) -> Tensor", 558},
      {"aten::var(Tensor self, int[1] dim, bool unbiased=True, bool keepdim=False, *, Tensor(a!) out) -> Tensor(a!)", 559},
      {"aten::var_mean(Tensor self, bool unbiased=True) -> (Tensor, Tensor)", 560},
      {"aten::var_mean(Tensor self, int[1] dim, bool unbiased=True, bool keepdim=False) -> (Tensor, Tensor)", 561},
      {"aten::view_as(Tensor self, Tensor other) -> Tensor", 562},
      {"aten::where(Tensor condition, Tensor self, Tensor other) -> Tensor", 563},
      {"aten::_s_where(Tensor condition, Tensor self, Tensor other) -> Tensor", 564},
      {"aten::norm_except_dim(Tensor v, int pow=2, int dim=0) -> Tensor", 565},
      {"aten::_weight_norm(Tensor v, Tensor g, int dim=0) -> Tensor", 566},
      {"aten::_weight_norm_cuda_interface(Tensor v, Tensor g, int dim=0) -> (Tensor, Tensor)", 567},
      {"aten::_weight_norm_cuda_interface_backward(Tensor grad_w, Tensor saved_v, Tensor saved_g, Tensor saved_norms, int dim) -> (Tensor, Tensor)", 568},
      {"aten::_weight_norm_differentiable_backward(Tensor grad_w, Tensor saved_v, Tensor saved_g, Tensor saved_norms, int dim) -> (Tensor, Tensor)", 569},
      {"aten::zeros(int[] size, *, ScalarType? dtype=None, Layout? layout=None, Device? device=None, bool? pin_memory=None) -> Tensor", 570},
      {"aten::zeros(int[] size, *, Tensor(a!) out) -> Tensor(a!)", 571},
      {"aten::zeros_like(Tensor self) -> Tensor", 572},
      {"aten::zeros_like(Tensor self, *, ScalarType dtype, Layout layout, Device device, bool pin_memory=False) -> Tensor", 573},
      {"aten::_standard_gamma_grad(Tensor self, Tensor output) -> Tensor", 574},
      {"aten::_standard_gamma(Tensor self, Generator? generator=None) -> Tensor", 575},
      {"aten::_sample_dirichlet(Tensor self, Generator? generator=None) -> Tensor", 576},
      {"aten::poisson(Tensor self, Generator? generator=None) -> Tensor", 577},
      {"aten::native_norm(Tensor self, Scalar p=2) -> Tensor", 578},
      {"aten::_sparse_sum(Tensor self) -> Tensor", 579},
      {"aten::_sparse_sum(Tensor self, *, ScalarType dtype) -> Tensor", 580},
      {"aten::_sparse_sum(Tensor self, int[1] dim) -> Tensor", 581},
      {"aten::_sparse_sum(Tensor self, int[1] dim, *, ScalarType dtype) -> Tensor", 582},
      {"aten::_sparse_sum_backward(Tensor grad, Tensor self, int[] dim) -> Tensor", 583},
      {"aten::norm(Tensor self, Scalar? p, *, ScalarType dtype) -> Tensor", 584},
      {"aten::norm(Tensor self, Scalar p=2) -> Tensor", 585},
      {"aten::norm(Tensor self, Scalar? p, int[1] dim, bool keepdim, *, ScalarType dtype) -> Tensor", 586},
      {"aten::norm(Tensor self, Scalar? p, int[1] dim, bool keepdim=False) -> Tensor", 587},
      {"aten::norm(Tensor self, Scalar? p, int[1] dim, bool keepdim, *, ScalarType dtype, Tensor(a!) out) -> Tensor(a!)", 588},
      {"aten::norm(Tensor self, Scalar? p, int[1] dim, bool keepdim=False, *, Tensor(a!) out) -> Tensor(a!)", 589},
      {"aten::frobenius_norm(Tensor self) -> Tensor", 590},
      {"aten::frobenius_norm(Tensor self, int[1] dim, bool keepdim=False) -> Tensor", 591},
      {"aten::frobenius_norm(Tensor self, int[1] dim, bool keepdim=False, *, Tensor(a!) out) -> Tensor(a!)", 592},
      {"aten::nuclear_norm(Tensor self, bool keepdim=False) -> Tensor", 593},
      {"aten::nuclear_norm(Tensor self, bool keepdim=False, *, Tensor(a!) out) -> Tensor(a!)", 594},
      {"aten::clone(Tensor self) -> Tensor", 595},
      {"aten::resize_as_(Tensor(a!) self, Tensor the_template) -> Tensor(a!)", 596},
      {"aten::pow(Tensor self, Scalar exponent, *, Tensor(a!) out) -> Tensor(a!)", 597},
      {"aten::pow(Tensor self, Scalar exponent) -> Tensor", 598},
      {"aten::zero_(Tensor(a!) self) -> Tensor(a!)", 599},
      {"aten::sub(Tensor self, Tensor other, *, Scalar alpha=1, Tensor(a!) out) -> Tensor(a!)", 600},
      {"aten::sub(Tensor self, Tensor other, *, Scalar alpha=1) -> Tensor", 601},
      {"aten::sub_(Tensor(a!) self, Tensor other, *, Scalar alpha=1) -> Tensor(a!)", 602},
      {"aten::sub(Tensor self, Scalar other, Scalar alpha=1) -> Tensor", 603},
      {"aten::sub_(Tensor(a!) self, Scalar other, Scalar alpha=1) -> Tensor(a!)", 604},
      {"aten::rsub(Tensor self, Tensor other, *, Scalar alpha=1) -> Tensor", 605},
      {"aten::rsub(Tensor self, Scalar other, Scalar alpha=1) -> Tensor", 606},
      {"aten::s_native_addmm(Tensor self, Tensor mat1, Tensor mat2, *, Scalar beta=1, Scalar alpha=1, Tensor(a!) out) -> Tensor(a!)", 607},
      {"aten::s_native_addmm(Tensor self, Tensor mat1, Tensor mat2, *, Scalar beta=1, Scalar alpha=1) -> Tensor", 608},
      {"aten::s_native_addmm_(Tensor(a!) self, Tensor mat1, Tensor mat2, *, Scalar beta=1, Scalar alpha=1) -> Tensor(a!)", 609},
      {"aten::_sparse_addmm(Tensor self, Tensor sparse, Tensor dense, *, Scalar beta=1, Scalar alpha=1) -> Tensor", 610},
      {"aten::addmm(Tensor self, Tensor mat1, Tensor mat2, *, Scalar beta=1, Scalar alpha=1, Tensor(a!) out) -> Tensor(a!)", 611},
      {"aten::addmm(Tensor self, Tensor mat1, Tensor mat2, *, Scalar beta=1, Scalar alpha=1) -> Tensor", 612},
      {"aten::addmm_(Tensor(a!) self, Tensor mat1, Tensor mat2, *, Scalar beta=1, Scalar alpha=1) -> Tensor(a!)", 613},
      {"aten::sparse_coo_tensor(int[] size, *, ScalarType dtype, Layout layout, Device device, bool pin_memory=False) -> Tensor", 614},
      {"aten::sparse_coo_tensor(Tensor indices, Tensor values, *, ScalarType? dtype=None, Layout? layout=None, Device? device=None, bool? pin_memory=None) -> Tensor", 615},
      {"aten::sparse_coo_tensor(Tensor indices, Tensor values, int[] size, *, ScalarType? dtype=None, Layout? layout=None, Device? device=None, bool? pin_memory=None) -> Tensor", 616},
      {"aten::_sparse_coo_tensor_unsafe(Tensor indices, Tensor values, int[] size, *, ScalarType? dtype=None, Layout? layout=None, Device? device=None, bool? pin_memory=None) -> Tensor", 617},
      {"aten::_sparse_coo_tensor_with_dims(int sparse_dim, int dense_dim, int[] size, *, ScalarType dtype, Layout layout, Device device, bool pin_memory=False) -> Tensor", 618},
      {"aten::_sparse_coo_tensor_with_dims_and_tensors(int sparse_dim, int dense_dim, int[] size, Tensor indices, Tensor values, *, ScalarType dtype, Layout layout, Device device, bool pin_memory=False) -> Tensor", 619},
      {"aten::sparse_resize_(Tensor(a!) self, int[] size, int sparse_dim, int dense_dim) -> Tensor(a!)", 620},
      {"aten::sparse_resize_and_clear_(Tensor(a!) self, int[] size, int sparse_dim, int dense_dim) -> Tensor(a!)", 621},
      {"aten::sparse_mask(Tensor self, Tensor mask) -> Tensor", 622},
      {"aten::to_dense(Tensor self) -> Tensor", 623},
      {"aten::to_dense_backward(Tensor grad, Tensor input) -> Tensor", 624},
      {"aten::sparse_dim(Tensor self) -> int", 625},
      {"aten::_dimI(Tensor self) -> int", 626},
      {"aten::dense_dim(Tensor self) -> int", 627},
      {"aten::_dimV(Tensor self) -> int", 628},
      {"aten::_nnz(Tensor self) -> int", 629},
      {"aten::coalesce(Tensor self) -> Tensor", 630},
      {"aten::is_coalesced(Tensor self) -> bool", 631},
      {"aten::_indices(Tensor(a) self) -> Tensor(a)", 632},
      {"aten::_values(Tensor(a) self) -> Tensor(a)", 633},
      {"aten::_coalesced_(Tensor(a!) self, bool coalesced) -> Tensor(a!)", 634},
      {"aten::indices(Tensor(a) self) -> Tensor(a)", 635},
      {"aten::values(Tensor(a) self) -> Tensor(a)", 636},
      {"aten::hspmm(Tensor mat1, Tensor mat2, *, Tensor(a!) out) -> Tensor(a!)", 637},
      {"aten::hspmm(Tensor mat1, Tensor mat2) -> Tensor", 638},
      {"aten::copy_sparse_to_sparse_(Tensor(a!) self, Tensor src, bool non_blocking=False) -> Tensor(a!)", 639},
      {"aten::numel(Tensor self) -> int", 640},
      {"aten::unbind(Tensor(a) self, int dim=0) -> Tensor(a)[]", 641},
      {"aten::to_sparse(Tensor self, int sparse_dim) -> Tensor", 642},
      {"aten::to_sparse(Tensor self) -> Tensor", 643},
      {"aten::to_mkldnn(Tensor self) -> Tensor", 644},
      {"aten::mkldnn_reorder_conv2d_weight(Tensor self, int[2] padding=0, int[2] stride=1, int[2] dilation=1, int groups=1) -> Tensor", 645},
      {"aten::to_mkldnn_backward(Tensor grad, Tensor input) -> Tensor", 646},
      {"aten::quantize_linear(Tensor self, float scale, int zero_point, ScalarType dtype) -> Tensor", 647},
      {"aten::quantize_linear_per_channel(Tensor self, Tensor scales, Tensor zero_points, int[] axis, ScalarType dtype) -> Tensor", 648},
      {"aten::dequantize(Tensor self) -> Tensor", 649},
      {"aten::_dequantize_linear(Tensor self, float scale, int zero_point, ScalarType dtype) -> Tensor", 650},
      {"aten::q_scale(Tensor self) -> Scalar", 651},
      {"aten::q_zero_point(Tensor self) -> Scalar", 652},
      {"aten::int_repr(Tensor self) -> Tensor", 653},
      {"aten::_per_tensor_affine_qtensor(Tensor self, float scale, int zero_point) -> Tensor", 654},
      {"aten::to(Tensor self, *, ScalarType dtype, Layout layout, Device device, bool pin_memory=False, bool non_blocking=False, bool copy=False) -> Tensor", 655},
      {"aten::to(Tensor self, Device device, ScalarType dtype, bool non_blocking=False, bool copy=False) -> Tensor", 656},
      {"aten::to(Tensor self, ScalarType dtype, bool non_blocking=False, bool copy=False) -> Tensor", 657},
      {"aten::to(Tensor self, Tensor other, bool non_blocking=False, bool copy=False) -> Tensor", 658},
      {"aten::meshgrid(Tensor[] tensors) -> Tensor[]", 659},
      {"aten::cartesian_prod(Tensor[] tensors) -> Tensor", 660},
      {"aten::combinations(Tensor self, int r=2, bool with_replacement=False) -> Tensor", 661},
      {"aten::item(Tensor self) -> Scalar", 662},
      {"aten::_local_scalar_dense(Tensor self) -> Scalar", 663},
      {"aten::_thnn_fused_lstm_cell(Tensor input_gates, Tensor hidden_gates, Tensor cx, Tensor? input_bias=None, Tensor? hidden_bias=None) -> (Tensor, Tensor, Tensor)", 664},
      {"aten::_thnn_fused_lstm_cell_backward(Tensor? grad_hy, Tensor? grad_cy, Tensor cx, Tensor cy, Tensor workspace, bool has_bias) -> (Tensor, Tensor, Tensor, Tensor, Tensor)", 665},
      {"aten::_thnn_fused_gru_cell(Tensor input_gates, Tensor hidden_gates, Tensor hx, Tensor? input_bias=None, Tensor? hidden_bias=None) -> (Tensor, Tensor)", 666},
      {"aten::_thnn_fused_gru_cell_backward(Tensor grad_hy, Tensor workspace, bool has_bias) -> (Tensor, Tensor, Tensor, Tensor, Tensor)", 667},
      {"aten::lstm(Tensor input, Tensor[] hx, Tensor[] params, bool has_biases, int num_layers, float dropout, bool train, bool bidirectional, bool batch_first) -> (Tensor, Tensor, Tensor)", 668},
      {"aten::lstm(Tensor data, Tensor batch_sizes, Tensor[] hx, Tensor[] params, bool has_biases, int num_layers, float dropout, bool train, bool bidirectional) -> (Tensor, Tensor, Tensor)", 669},
      {"aten::gru(Tensor input, Tensor hx, Tensor[] params, bool has_biases, int num_layers, float dropout, bool train, bool bidirectional, bool batch_first) -> (Tensor, Tensor)", 670},
      {"aten::gru(Tensor data, Tensor batch_sizes, Tensor hx, Tensor[] params, bool has_biases, int num_layers, float dropout, bool train, bool bidirectional) -> (Tensor, Tensor)", 671},
      {"aten::rnn_tanh(Tensor input, Tensor hx, Tensor[] params, bool has_biases, int num_layers, float dropout, bool train, bool bidirectional, bool batch_first) -> (Tensor, Tensor)", 672},
      {"aten::rnn_tanh(Tensor data, Tensor batch_sizes, Tensor hx, Tensor[] params, bool has_biases, int num_layers, float dropout, bool train, bool bidirectional) -> (Tensor, Tensor)", 673},
      {"aten::rnn_relu(Tensor input, Tensor hx, Tensor[] params, bool has_biases, int num_layers, float dropout, bool train, bool bidirectional, bool batch_first) -> (Tensor, Tensor)", 674},
      {"aten::rnn_relu(Tensor data, Tensor batch_sizes, Tensor hx, Tensor[] params, bool has_biases, int num_layers, float dropout, bool train, bool bidirectional) -> (Tensor, Tensor)", 675},
      {"aten::lstm_cell(Tensor input, Tensor[] hx, Tensor w_ih, Tensor w_hh, Tensor? b_ih=None, Tensor? b_hh=None) -> (Tensor, Tensor)", 676},
      {"aten::gru_cell(Tensor input, Tensor hx, Tensor w_ih, Tensor w_hh, Tensor? b_ih=None, Tensor? b_hh=None) -> Tensor", 677},
      {"aten::rnn_tanh_cell(Tensor input, Tensor hx, Tensor w_ih, Tensor w_hh, Tensor? b_ih=None, Tensor? b_hh=None) -> Tensor", 678},
      {"aten::rnn_relu_cell(Tensor input, Tensor hx, Tensor w_ih, Tensor w_hh, Tensor? b_ih=None, Tensor? b_hh=None) -> Tensor", 679},
      {"aten::quantized_lstm(Tensor input, Tensor[] hx, Tensor[] params, bool has_biases, int num_layers, float dropout, bool train, bool bidirectional, bool batch_first) -> (Tensor, Tensor, Tensor)", 680},
      {"aten::quantized_lstm_cell(Tensor input, Tensor[] hx, Tensor w_ih, Tensor w_hh, Tensor b_ih, Tensor b_hh, Tensor packed_ih, Tensor packed_hh, Tensor col_offsets_ih, Tensor col_offsets_hh, Scalar scale_ih, Scalar scale_hh, Scalar zero_point_ih, Scalar zero_point_hh) -> (Tensor, Tensor)", 681},
      {"aten::quantized_gru_cell(Tensor input, Tensor hx, Tensor w_ih, Tensor w_hh, Tensor b_ih, Tensor b_hh, Tensor packed_ih, Tensor packed_hh, Tensor col_offsets_ih, Tensor col_offsets_hh, Scalar scale_ih, Scalar scale_hh, Scalar zero_point_ih, Scalar zero_point_hh) -> Tensor", 682},
      {"aten::quantized_rnn_relu_cell(Tensor input, Tensor hx, Tensor w_ih, Tensor w_hh, Tensor b_ih, Tensor b_hh, Tensor packed_ih, Tensor packed_hh, Tensor col_offsets_ih, Tensor col_offsets_hh, Scalar scale_ih, Scalar scale_hh, Scalar zero_point_ih, Scalar zero_point_hh) -> Tensor", 683},
      {"aten::quantized_rnn_tanh_cell(Tensor input, Tensor hx, Tensor w_ih, Tensor w_hh, Tensor b_ih, Tensor b_hh, Tensor packed_ih, Tensor packed_hh, Tensor col_offsets_ih, Tensor col_offsets_hh, Scalar scale_ih, Scalar scale_hh, Scalar zero_point_ih, Scalar zero_point_hh) -> Tensor", 684},
      {"aten::_pack_padded_sequence(Tensor input, Tensor lengths, bool batch_first) -> (Tensor, Tensor)", 685},
      {"aten::_pack_padded_sequence_backward(Tensor grad, int[] input_size, Tensor batch_sizes, bool batch_first) -> Tensor", 686},
      {"aten::_pad_packed_sequence(Tensor data, Tensor batch_sizes, bool batch_first, Scalar padding_value, int total_length) -> (Tensor, Tensor)", 687},
      {"aten::set_(Tensor(a!) self, Storage source) -> Tensor(a!)", 688},
      {"aten::set_(Tensor(a!) self, Storage source, int storage_offset, int[] size, int[] stride=[]) -> Tensor(a!)", 689},
      {"aten::set_(Tensor(a!) self, Tensor source) -> Tensor(a!)", 690},
      {"aten::set_(Tensor(a!) self) -> Tensor(a!)", 691},
      {"aten::is_set_to(Tensor self, Tensor tensor) -> bool", 692},
      {"aten::masked_fill_(Tensor(a!) self, Tensor mask, Scalar value) -> Tensor(a!)", 693},
      {"aten::masked_fill(Tensor self, Tensor mask, Scalar value) -> Tensor", 694},
      {"aten::masked_fill_(Tensor(a!) self, Tensor mask, Tensor value) -> Tensor(a!)", 695},
      {"aten::masked_fill(Tensor self, Tensor mask, Tensor value) -> Tensor", 696},
      {"aten::masked_scatter_(Tensor(a!) self, Tensor mask, Tensor source) -> Tensor(a!)", 697},
      {"aten::masked_scatter(Tensor self, Tensor mask, Tensor source) -> Tensor", 698},
      {"aten::view(Tensor(a) self, int[] size) -> Tensor(a)", 699},
      {"aten::put_(Tensor(a!) self, Tensor index, Tensor source, bool accumulate=False) -> Tensor(a!)", 700},
      {"aten::index_add_(Tensor(a!) self, int dim, Tensor index, Tensor source) -> Tensor(a!)", 701},
      {"aten::index_add(Tensor self, int dim, Tensor index, Tensor source) -> Tensor", 702},
      {"aten::index_fill_(Tensor(a!) self, int dim, Tensor index, Scalar value) -> Tensor(a!)", 703},
      {"aten::index_fill(Tensor self, int dim, Tensor index, Scalar value) -> Tensor", 704},
      {"aten::index_fill_(Tensor(a!) self, int dim, Tensor index, Tensor value) -> Tensor(a!)", 705},
      {"aten::index_fill(Tensor self, int dim, Tensor index, Tensor value) -> Tensor", 706},
      {"aten::scatter_(Tensor(a!) self, int dim, Tensor index, Tensor src) -> Tensor(a!)", 707},
      {"aten::scatter(Tensor self, int dim, Tensor index, Tensor src) -> Tensor", 708},
      {"aten::scatter_(Tensor(a!) self, int dim, Tensor index, Scalar value) -> Tensor(a!)", 709},
      {"aten::scatter(Tensor self, int dim, Tensor index, Scalar value) -> Tensor", 710},
      {"aten::scatter_add_(Tensor(a!) self, int dim, Tensor index, Tensor src) -> Tensor(a!)", 711},
      {"aten::scatter_add(Tensor self, int dim, Tensor index, Tensor src) -> Tensor", 712},
      {"aten::lt_(Tensor(a!) self, Scalar other) -> Tensor(a!)", 713},
      {"aten::lt_(Tensor(a!) self, Tensor other) -> Tensor(a!)", 714},
      {"aten::gt_(Tensor(a!) self, Scalar other) -> Tensor(a!)", 715},
      {"aten::gt_(Tensor(a!) self, Tensor other) -> Tensor(a!)", 716},
      {"aten::le_(Tensor(a!) self, Scalar other) -> Tensor(a!)", 717},
      {"aten::le_(Tensor(a!) self, Tensor other) -> Tensor(a!)", 718},
      {"aten::ge_(Tensor(a!) self, Scalar other) -> Tensor(a!)", 719},
      {"aten::ge_(Tensor(a!) self, Tensor other) -> Tensor(a!)", 720},
      {"aten::eq_(Tensor(a!) self, Scalar other) -> Tensor(a!)", 721},
      {"aten::eq_(Tensor(a!) self, Tensor other) -> Tensor(a!)", 722},
      {"aten::ne_(Tensor(a!) self, Scalar other) -> Tensor(a!)", 723},
      {"aten::ne_(Tensor(a!) self, Tensor other) -> Tensor(a!)", 724},
      {"aten::__and__(Tensor self, Scalar other) -> Tensor", 725},
      {"aten::__and__(Tensor self, Tensor other) -> Tensor", 726},
      {"aten::__iand__(Tensor(a!) self, Scalar other) -> Tensor(a!)", 727},
      {"aten::__iand__(Tensor(a!) self, Tensor other) -> Tensor(a!)", 728},
      {"aten::__or__(Tensor self, Scalar other) -> Tensor", 729},
      {"aten::__or__(Tensor self, Tensor other) -> Tensor", 730},
      {"aten::__ior__(Tensor(a!) self, Scalar other) -> Tensor(a!)", 731},
      {"aten::__ior__(Tensor(a!) self, Tensor other) -> Tensor(a!)", 732},
      {"aten::__xor__(Tensor self, Scalar other) -> Tensor", 733},
      {"aten::__xor__(Tensor self, Tensor other) -> Tensor", 734},
      {"aten::__ixor__(Tensor(a!) self, Scalar other) -> Tensor(a!)", 735},
      {"aten::__ixor__(Tensor(a!) self, Tensor other) -> Tensor(a!)", 736},
      {"aten::__lshift__(Tensor self, Scalar other) -> Tensor", 737},
      {"aten::__lshift__(Tensor self, Tensor other) -> Tensor", 738},
      {"aten::__ilshift__(Tensor(a!) self, Scalar other) -> Tensor(a!)", 739},
      {"aten::__ilshift__(Tensor(a!) self, Tensor other) -> Tensor(a!)", 740},
      {"aten::__rshift__(Tensor self, Scalar other) -> Tensor", 741},
      {"aten::__rshift__(Tensor self, Tensor other) -> Tensor", 742},
      {"aten::__irshift__(Tensor(a!) self, Scalar other) -> Tensor(a!)", 743},
      {"aten::__irshift__(Tensor(a!) self, Tensor other) -> Tensor(a!)", 744},
      {"aten::lgamma_(Tensor(a!) self) -> Tensor(a!)", 745},
      {"aten::atan2_(Tensor(a!) self, Tensor other) -> Tensor(a!)", 746},
      {"aten::tril_(Tensor(a!) self, int diagonal=0) -> Tensor(a!)", 747},
      {"aten::triu_(Tensor(a!) self, int diagonal=0) -> Tensor(a!)", 748},
      {"aten::digamma_(Tensor(a!) self) -> Tensor(a!)", 749},
      {"aten::polygamma_(Tensor(a!) self, int n) -> Tensor(a!)", 750},
      {"aten::erfinv_(Tensor(a!) self) -> Tensor(a!)", 751},
      {"aten::renorm_(Tensor(a!) self, Scalar p, int dim, Scalar maxnorm) -> Tensor(a!)", 752},
      {"aten::pow_(Tensor(a!) self, Scalar exponent) -> Tensor(a!)", 753},
      {"aten::pow_(Tensor(a!) self, Tensor exponent) -> Tensor(a!)", 754},
      {"aten::lerp_(Tensor(a!) self, Tensor end, Scalar weight) -> Tensor(a!)", 755},
      {"aten::lerp_(Tensor(a!) self, Tensor end, Tensor weight) -> Tensor(a!)", 756},
      {"aten::sign_(Tensor(a!) self) -> Tensor(a!)", 757},
      {"aten::fmod_(Tensor(a!) self, Scalar other) -> Tensor(a!)", 758},
      {"aten::fmod_(Tensor(a!) self, Tensor other) -> Tensor(a!)", 759},
      {"aten::remainder_(Tensor(a!) self, Scalar other) -> Tensor(a!)", 760},
      {"aten::remainder_(Tensor(a!) self, Tensor other) -> Tensor(a!)", 761},
      {"aten::addbmm_(Tensor(a!) self, Tensor batch1, Tensor batch2, *, Scalar beta=1, Scalar alpha=1) -> Tensor(a!)", 762},
      {"aten::addbmm(Tensor self, Tensor batch1, Tensor batch2, *, Scalar beta=1, Scalar alpha=1, Tensor(a!) out) -> Tensor(a!)", 763},
      {"aten::addbmm(Tensor self, Tensor batch1, Tensor batch2, *, Scalar beta=1, Scalar alpha=1) -> Tensor", 764},
      {"aten::addcmul_(Tensor(a!) self, Tensor tensor1, Tensor tensor2, *, Scalar value=1) -> Tensor(a!)", 765},
      {"aten::addcdiv_(Tensor(a!) self, Tensor tensor1, Tensor tensor2, *, Scalar value=1) -> Tensor(a!)", 766},
      {"aten::random_(Tensor(a!) self, int from, int to, *, Generator? generator=None) -> Tensor(a!)", 767},
      {"aten::random_(Tensor(a!) self, int to, *, Generator? generator=None) -> Tensor(a!)", 768},
      {"aten::random_(Tensor(a!) self, *, Generator? generator=None) -> Tensor(a!)", 769},
      {"aten::uniform_(Tensor(a!) self, float from=0, float to=1, *, Generator? generator=None) -> Tensor(a!)", 770},
      {"aten::normal_(Tensor(a!) self, float mean=0, float std=1, *, Generator? generator=None) -> Tensor(a!)", 771},
      {"aten::cauchy_(Tensor(a!) self, float median=0, float sigma=1, *, Generator? generator=None) -> Tensor(a!)", 772},
      {"aten::log_normal_(Tensor(a!) self, float mean=1, float std=2, *, Generator? generator=None) -> Tensor(a!)", 773},
      {"aten::exponential_(Tensor(a!) self, float lambd=1, *, Generator? generator=None) -> Tensor(a!)", 774},
      {"aten::geometric_(Tensor(a!) self, float p, *, Generator? generator=None) -> Tensor(a!)", 775},
      {"aten::diag(Tensor self, int diagonal=0, *, Tensor(a!) out) -> Tensor(a!)", 776},
      {"aten::diag(Tensor self, int diagonal=0) -> Tensor", 777},
      {"aten::cross(Tensor self, Tensor other, int? dim=None, *, Tensor(a!) out) -> Tensor(a!)", 778},
      {"aten::cross(Tensor self, Tensor other, int? dim=None) -> Tensor", 779},
      {"aten::triu(Tensor self, int diagonal=0, *, Tensor(a!) out) -> Tensor(a!)", 780},
      {"aten::triu(Tensor self, int diagonal=0) -> Tensor", 781},
      {"aten::tril(Tensor self, int diagonal=0, *, Tensor(a!) out) -> Tensor(a!)", 782},
      {"aten::tril(Tensor self, int diagonal=0) -> Tensor", 783},
      {"aten::tril_indices(int row, int col, int offset=0, *, ScalarType? dtype=long, Layout? layout=None, Device? device=None, bool? pin_memory=None) -> Tensor", 784},
      {"aten::triu_indices(int row, int col, int offset=0, *, ScalarType? dtype=long, Layout? layout=None, Device? device=None, bool? pin_memory=None) -> Tensor", 785},
      {"aten::trace(Tensor self) -> Tensor", 786},
      {"aten::ne(Tensor self, Scalar other, *, Tensor(a!) out) -> Tensor(a!)", 787},
      {"aten::ne(Tensor self, Scalar other) -> Tensor", 788},
      {"aten::ne(Tensor self, Tensor other, *, Tensor(a!) out) -> Tensor(a!)", 789},
      {"aten::ne(Tensor self, Tensor other) -> Tensor", 790},
      {"aten::eq(Tensor self, Scalar other, *, Tensor(a!) out) -> Tensor(a!)", 791},
      {"aten::eq(Tensor self, Scalar other) -> Tensor", 792},
      {"aten::eq(Tensor self, Tensor other, *, Tensor(a!) out) -> Tensor(a!)", 793},
      {"aten::eq(Tensor self, Tensor other) -> Tensor", 794},
      {"aten::ge(Tensor self, Scalar other, *, Tensor(a!) out) -> Tensor(a!)", 795},
      {"aten::ge(Tensor self, Scalar other) -> Tensor", 796},
      {"aten::ge(Tensor self, Tensor other, *, Tensor(a!) out) -> Tensor(a!)", 797},
      {"aten::ge(Tensor self, Tensor other) -> Tensor", 798},
      {"aten::le(Tensor self, Scalar other, *, Tensor(a!) out) -> Tensor(a!)", 799},
      {"aten::le(Tensor self, Scalar other) -> Tensor", 800},
      {"aten::le(Tensor self, Tensor other, *, Tensor(a!) out) -> Tensor(a!)", 801},
      {"aten::le(Tensor self, Tensor other) -> Tensor", 802},
      {"aten::gt(Tensor self, Scalar other, *, Tensor(a!) out) -> Tensor(a!)", 803},
      {"aten::gt(Tensor self, Scalar other) -> Tensor", 804},
      {"aten::gt(Tensor self, Tensor other, *, Tensor(a!) out) -> Tensor(a!)", 805},
      {"aten::gt(Tensor self, Tensor other) -> Tensor", 806},
      {"aten::lt(Tensor self, Scalar other, *, Tensor(a!) out) -> Tensor(a!)", 807},
      {"aten::lt(Tensor self, Scalar other) -> Tensor", 808},
      {"aten::lt(Tensor self, Tensor other, *, Tensor(a!) out) -> Tensor(a!)", 809},
      {"aten::lt(Tensor self, Tensor other) -> Tensor", 810},
      {"aten::take(Tensor self, Tensor index, *, Tensor(a!) out) -> Tensor(a!)", 811},
      {"aten::take(Tensor self, Tensor index) -> Tensor", 812},
      {"aten::index_select(Tensor self, int dim, Tensor index, *, Tensor(a!) out) -> Tensor(a!)", 813},
      {"aten::index_select(Tensor self, int dim, Tensor index) -> Tensor", 814},
      {"aten::masked_select(Tensor self, Tensor mask, *, Tensor(a!) out) -> Tensor(a!)", 815},
      {"aten::masked_select(Tensor self, Tensor mask) -> Tensor", 816},
      {"aten::nonzero(Tensor self, *, Tensor(a!) out) -> Tensor(a!)", 817},
      {"aten::nonzero(Tensor self) -> Tensor", 818},
      {"aten::gather(Tensor self, int dim, Tensor index, *, bool sparse_grad=False, Tensor(a!) out) -> Tensor(a!)", 819},
      {"aten::gather(Tensor self, int dim, Tensor index, *, bool sparse_grad=False) -> Tensor", 820},
      {"aten::_gather_sparse_backward(Tensor self, int dim, Tensor index, Tensor grad) -> Tensor", 821},
      {"aten::addcmul(Tensor self, Tensor tensor1, Tensor tensor2, *, Scalar value=1, Tensor(a!) out) -> Tensor(a!)", 822},
      {"aten::addcmul(Tensor self, Tensor tensor1, Tensor tensor2, *, Scalar value=1) -> Tensor", 823},
      {"aten::addcdiv(Tensor self, Tensor tensor1, Tensor tensor2, *, Scalar value=1, Tensor(a!) out) -> Tensor(a!)", 824},
      {"aten::addcdiv(Tensor self, Tensor tensor1, Tensor tensor2, *, Scalar value=1) -> Tensor", 825},
      {"aten::gels(Tensor self, Tensor A, *, Tensor(a!) X, Tensor(b!) qr) -> (Tensor(a!) solution, Tensor(b!) QR)", 826},
      {"aten::gels(Tensor self, Tensor A) -> (Tensor solution, Tensor QR)", 827},
      {"aten::triangular_solve(Tensor self, Tensor A, bool upper=True, bool transpose=False, bool unitriangular=False, *, Tensor(a!) X, Tensor(b!) M) -> (Tensor(a!) solution, Tensor(b!) cloned_coefficient)", 828},
      {"aten::triangular_solve(Tensor self, Tensor A, bool upper=True, bool transpose=False, bool unitriangular=False) -> (Tensor solution, Tensor cloned_coefficient)", 829},
      {"aten::_triangular_solve_helper(Tensor self, Tensor A, bool upper, bool transpose, bool unitriangular) -> (Tensor, Tensor)", 830},
      {"aten::symeig(Tensor self, bool eigenvectors=False, bool upper=True, *, Tensor(a!) e, Tensor(b!) V) -> (Tensor(a!) eigenvalues, Tensor(b!) eigenvectors)", 831},
      {"aten::symeig(Tensor self, bool eigenvectors=False, bool upper=True) -> (Tensor eigenvalues, Tensor eigenvectors)", 832},
      {"aten::eig(Tensor self, bool eigenvectors=False, *, Tensor(a!) e, Tensor(b!) v) -> (Tensor(a!) eigenvalues, Tensor(b!) eigenvectors)", 833},
      {"aten::eig(Tensor self, bool eigenvectors=False) -> (Tensor eigenvalues, Tensor eigenvectors)", 834},
      {"aten::svd(Tensor self, bool some=True, bool compute_uv=True, *, Tensor(a!) U, Tensor(b!) S, Tensor(c!) V) -> (Tensor(a!) U, Tensor(b!) S, Tensor(c!) V)", 835},
      {"aten::svd(Tensor self, bool some=True, bool compute_uv=True) -> (Tensor U, Tensor S, Tensor V)", 836},
      {"aten::cholesky(Tensor self, bool upper=False, *, Tensor(a!) out) -> Tensor(a!)", 837},
      {"aten::cholesky(Tensor self, bool upper=False) -> Tensor", 838},
      {"aten::_cholesky_helper(Tensor self, bool upper) -> Tensor", 839},
      {"aten::cholesky_solve(Tensor self, Tensor input2, bool upper=False, *, Tensor(a!) out) -> Tensor(a!)", 840},
      {"aten::cholesky_solve(Tensor self, Tensor input2, bool upper=False) -> Tensor", 841},
      {"aten::_cholesky_solve_helper(Tensor self, Tensor A, bool upper) -> Tensor", 842},
      {"aten::solve(Tensor self, Tensor A) -> (Tensor solution, Tensor LU)", 843},
      {"aten::solve(Tensor self, Tensor A, *, Tensor(a!) solution, Tensor(b!) lu) -> (Tensor(a!) solution, Tensor(b!) LU)", 844},
      {"aten::_solve_helper(Tensor self, Tensor A) -> (Tensor, Tensor)", 845},
      {"aten::cholesky_inverse(Tensor self, bool upper=False, *, Tensor(a!) out) -> Tensor(a!)", 846},
      {"aten::cholesky_inverse(Tensor self, bool upper=False) -> Tensor", 847},
      {"aten::pstrf(Tensor self, bool upper=True, Scalar tol=-1, *, Tensor(a!) u, Tensor(b!) pivot) -> (Tensor(a!) u, Tensor(b!) pivot)", 848},
      {"aten::pstrf(Tensor self, bool upper=True, Scalar tol=-1) -> (Tensor u, Tensor pivot)", 849},
      {"aten::qr(Tensor self, bool some=True, *, Tensor(a!) Q, Tensor(b!) R) -> (Tensor(a!) Q, Tensor(b!) R)", 850},
      {"aten::qr(Tensor self, bool some=True) -> (Tensor Q, Tensor R)", 851},
      {"aten::_qr_helper(Tensor self, bool some) -> (Tensor, Tensor)", 852},
      {"aten::geqrf(Tensor self, *, Tensor(a!) a, Tensor(b!) tau) -> (Tensor(a!) a, Tensor(b!) tau)", 853},
      {"aten::geqrf(Tensor self) -> (Tensor a, Tensor tau)", 854},
      {"aten::orgqr(Tensor self, Tensor input2, *, Tensor(a!) out) -> Tensor(a!)", 855},
      {"aten::orgqr(Tensor self, Tensor input2) -> Tensor", 856},
      {"aten::ormqr(Tensor self, Tensor input2, Tensor input3, bool left=True, bool transpose=False, *, Tensor(a!) out) -> Tensor(a!)", 857},
      {"aten::ormqr(Tensor self, Tensor input2, Tensor input3, bool left=True, bool transpose=False) -> Tensor", 858},
      {"aten::_lu_with_info(Tensor self, bool pivot=True, bool check_errors=True) -> (Tensor, Tensor, Tensor)", 859},
      {"aten::lu_solve(Tensor self, Tensor LU_data, Tensor LU_pivots, *, Tensor(a!) out) -> Tensor(a!)", 860},
      {"aten::lu_solve(Tensor self, Tensor LU_data, Tensor LU_pivots) -> Tensor", 861},
      {"aten::multinomial(Tensor self, int num_samples, bool replacement=False, *, Generator? generator=None, Tensor(a!) out) -> Tensor(a!)", 862},
      {"aten::multinomial(Tensor self, int num_samples, bool replacement=False, *, Generator? generator=None) -> Tensor", 863},
      {"aten::_multinomial_alias_setup(Tensor probs) -> (Tensor, Tensor)", 864},
      {"aten::_multinomial_alias_draw(Tensor J, Tensor q, int num_samples, *, Generator? generator=None) -> Tensor", 865},
      {"aten::lgamma(Tensor self, *, Tensor(a!) out) -> Tensor(a!)", 866},
      {"aten::lgamma(Tensor self) -> Tensor", 867},
      {"aten::digamma(Tensor self, *, Tensor(a!) out) -> Tensor(a!)", 868},
      {"aten::digamma(Tensor self) -> Tensor", 869},
      {"aten::polygamma(int n, Tensor self, *, Tensor(a!) out) -> Tensor(a!)", 870},
      {"aten::polygamma(int n, Tensor self) -> Tensor", 871},
      {"aten::erfinv(Tensor self, *, Tensor(a!) out) -> Tensor(a!)", 872},
      {"aten::erfinv(Tensor self) -> Tensor", 873},
      {"aten::dist(Tensor self, Tensor other, Scalar p=2) -> Tensor", 874},
      {"aten::atan2(Tensor self, Tensor other, *, Tensor(a!) out) -> Tensor(a!)", 875},
      {"aten::atan2(Tensor self, Tensor other) -> Tensor", 876},
      {"aten::lerp(Tensor self, Tensor end, Scalar weight, *, Tensor(a!) out) -> Tensor(a!)", 877},
      {"aten::lerp(Tensor self, Tensor end, Tensor weight, *, Tensor(a!) out) -> Tensor(a!)", 878},
      {"aten::lerp(Tensor self, Tensor end, Scalar weight) -> Tensor", 879},
      {"aten::lerp(Tensor self, Tensor end, Tensor weight) -> Tensor", 880},
      {"aten::histc(Tensor self, int bins=100, Scalar min=0, Scalar max=0, *, Tensor(a!) out) -> Tensor(a!)", 881},
      {"aten::histc(Tensor self, int bins=100, Scalar min=0, Scalar max=0) -> Tensor", 882},
      {"aten::sign(Tensor self, *, Tensor(a!) out) -> Tensor(a!)", 883},
      {"aten::sign(Tensor self) -> Tensor", 884},
      {"aten::fmod(Tensor self, Scalar other, *, Tensor(a!) out) -> Tensor(a!)", 885},
      {"aten::fmod(Tensor self, Scalar other) -> Tensor", 886},
      {"aten::fmod(Tensor self, Tensor other, *, Tensor(a!) out) -> Tensor(a!)", 887},
      {"aten::fmod(Tensor self, Tensor other) -> Tensor", 888},
      {"aten::remainder(Tensor self, Scalar other, *, Tensor(a!) out) -> Tensor(a!)", 889},
      {"aten::remainder(Tensor self, Scalar other) -> Tensor", 890},
      {"aten::remainder(Tensor self, Tensor other, *, Tensor(a!) out) -> Tensor(a!)", 891},
      {"aten::remainder(Tensor self, Tensor other) -> Tensor", 892},
      {"aten::min(Tensor self, Tensor other, *, Tensor(a!) out) -> Tensor(a!)", 893},
      {"aten::min(Tensor self, Tensor other) -> Tensor", 894},
      {"aten::min(Tensor self) -> Tensor", 895},
      {"aten::max(Tensor self, Tensor other, *, Tensor(a!) out) -> Tensor(a!)", 896},
      {"aten::max(Tensor self, Tensor other) -> Tensor", 897},
      {"aten::max(Tensor self) -> Tensor", 898},
      {"aten::median(Tensor self) -> Tensor", 899},
      {"aten::sort(Tensor self, int dim=-1, bool descending=False, *, Tensor(a!) values, Tensor(b!) indices) -> (Tensor(a!) values, Tensor(b!) indices)", 900},
      {"aten::sort(Tensor self, int dim=-1, bool descending=False) -> (Tensor values, Tensor indices)", 901},
      {"aten::argsort(Tensor self, int dim=-1, bool descending=False) -> Tensor", 902},
      {"aten::topk(Tensor self, int k, int dim=-1, bool largest=True, bool sorted=True, *, Tensor(a!) values, Tensor(b!) indices) ->(Tensor(a!) values, Tensor(b!) indices)", 903},
      {"aten::topk(Tensor self, int k, int dim=-1, bool largest=True, bool sorted=True) -> (Tensor values, Tensor indices)", 904},
      {"aten::all(Tensor self) -> Tensor", 905},
      {"aten::any(Tensor self) -> Tensor", 906},
      {"aten::renorm(Tensor self, Scalar p, int dim, Scalar maxnorm, *, Tensor(a!) out) -> Tensor(a!)", 907},
      {"aten::renorm(Tensor self, Scalar p, int dim, Scalar maxnorm) -> Tensor", 908},
      {"aten::unfold(Tensor(a) self, int dimension, int size, int step) -> Tensor(a)", 909},
      {"aten::equal(Tensor self, Tensor other) -> bool", 910},
      {"aten::pow(Tensor self, Tensor exponent, *, Tensor(a!) out) -> Tensor(a!)", 911},
      {"aten::pow(Tensor self, Tensor exponent) -> Tensor", 912},
      {"aten::pow(Scalar self, Tensor exponent, *, Tensor(a!) out) -> Tensor(a!)", 913},
      {"aten::pow(Scalar self, Tensor exponent) -> Tensor", 914},
      {"aten::normal(Tensor mean, float std=1, *, Generator? generator=None, Tensor(a!) out) -> Tensor(a!)", 915},
      {"aten::normal(Tensor mean, float std=1, *, Generator? generator=None) -> Tensor", 916},
      {"aten::normal(float mean, Tensor std, *, Generator? generator=None, Tensor(a!) out) -> Tensor(a!)", 917},
      {"aten::normal(float mean, Tensor std, *, Generator? generator=None) -> Tensor", 918},
      {"aten::normal(Tensor mean, Tensor std, *, Generator? generator=None, Tensor(a!) out) -> Tensor(a!)", 919},
      {"aten::normal(Tensor mean, Tensor std, *, Generator? generator=None) -> Tensor", 920},
      {"aten::alias(Tensor(a) self) -> Tensor(a)", 921},
      {"aten::_dirichlet_grad(Tensor x, Tensor alpha, Tensor total, *, Tensor(a!) out) -> Tensor(a!)", 922},
      {"aten::_dirichlet_grad(Tensor x, Tensor alpha, Tensor total) -> Tensor", 923},
      {"aten::_addr(Tensor self, Tensor vec1, Tensor vec2, *, Scalar beta=1, Scalar alpha=1) -> Tensor", 924},
      {"aten::_addr_(Tensor(a!) self, Tensor vec1, Tensor vec2, *, Scalar beta=1, Scalar alpha=1) -> Tensor(a!)", 925},
      {"aten::_addr(Tensor self, Tensor vec1, Tensor vec2, *, Scalar beta=1, Scalar alpha=1, Tensor(a!) out) -> Tensor(a!)", 926},
      {"aten::_index_copy_(Tensor(a!) self, int dim, Tensor index, Tensor source) -> Tensor(a!)", 927},
      {"aten::_cumsum(Tensor self, int dim) -> Tensor", 928},
      {"aten::_cumsum(Tensor self, int dim, *, Tensor(a!) out) -> Tensor(a!)", 929},
      {"aten::_cumprod(Tensor self, int dim) -> Tensor", 930},
      {"aten::_cumprod(Tensor self, int dim, *, Tensor(a!) out) -> Tensor(a!)", 931},
      {"aten::_var(Tensor self, bool unbiased=True) -> Tensor", 932},
      {"aten::_std(Tensor self, bool unbiased=True) -> Tensor", 933},
      {"aten::_addmm(Tensor self, Tensor mat1, Tensor mat2, *, Scalar beta=1, Scalar alpha=1, Tensor(a!) out) -> Tensor(a!)", 934},
      {"aten::_addmm(Tensor self, Tensor mat1, Tensor mat2, *, Scalar beta=1, Scalar alpha=1) -> Tensor", 935},
      {"aten::_addmm_(Tensor(a!) self, Tensor mat1, Tensor mat2, *, Scalar beta=1, Scalar alpha=1) -> Tensor(a!)", 936},
      {"aten::_cat(Tensor[] tensors, int dim=0) -> Tensor", 937},
      {"aten::_cat(Tensor[] tensors, int dim=0, *, Tensor(a!) out) -> Tensor(a!)", 938},
      {"aten::_mode(Tensor self, int dim=-1, bool keepdim=False) -> (Tensor, Tensor)", 939},
      {"aten::_mode(Tensor self, int dim=-1, bool keepdim=False, *, Tensor(a!) values, Tensor(b!) indices) -> (Tensor(a!), Tensor(b!))", 940},
      {"aten::_max(Tensor self, int dim, bool keepdim=False) -> (Tensor, Tensor)", 941},
      {"aten::_max(Tensor self, int dim, bool keepdim=False, *, Tensor(a!) max, Tensor(b!) max_indices) -> (Tensor(a!), Tensor(b!))", 942},
      {"aten::_min(Tensor self, int dim, bool keepdim=False) -> (Tensor, Tensor)", 943},
      {"aten::_min(Tensor self, int dim, bool keepdim=False, *, Tensor(a!) min, Tensor(b!) min_indices) -> (Tensor(a!), Tensor(b!))", 944},
      {"aten::binary_cross_entropy(Tensor self, Tensor target, Tensor? weight=None, int reduction=Mean, *, Tensor(a!) out) -> Tensor(a!)", 945},
      {"aten::binary_cross_entropy(Tensor self, Tensor target, Tensor? weight=None, int reduction=Mean) -> Tensor", 946},
      {"aten::binary_cross_entropy_backward(Tensor grad_output, Tensor self, Tensor target, Tensor? weight=None, int reduction=Mean, *, Tensor(a!) grad_input) -> Tensor(a!)", 947},
      {"aten::binary_cross_entropy_backward(Tensor grad_output, Tensor self, Tensor target, Tensor? weight=None, int reduction=Mean) -> Tensor", 948},
      {"aten::mse_loss(Tensor self, Tensor target, int reduction=Mean, *, Tensor(a!) out) -> Tensor(a!)", 949},
      {"aten::mse_loss(Tensor self, Tensor target, int reduction=Mean) -> Tensor", 950},
      {"aten::mse_loss_backward(Tensor grad_output, Tensor self, Tensor target, int reduction, *, Tensor(a!) grad_input) -> Tensor(a!)", 951},
      {"aten::mse_loss_backward(Tensor grad_output, Tensor self, Tensor target, int reduction) -> Tensor", 952},
      {"aten::l1_loss(Tensor self, Tensor target, int reduction=Mean, *, Tensor(a!) out) -> Tensor(a!)", 953},
      {"aten::l1_loss(Tensor self, Tensor target, int reduction=Mean) -> Tensor", 954},
      {"aten::l1_loss_backward(Tensor grad_output, Tensor self, Tensor target, int reduction, *, Tensor(a!) grad_input) -> Tensor(a!)", 955},
      {"aten::l1_loss_backward(Tensor grad_output, Tensor self, Tensor target, int reduction) -> Tensor", 956},
      {"aten::multi_margin_loss(Tensor self, Tensor target, Scalar p=1, Scalar margin=1, Tensor? weight=None, int reduction=Mean, *, Tensor(a!) out) -> Tensor(a!)", 957},
      {"aten::multi_margin_loss(Tensor self, Tensor target, Scalar p=1, Scalar margin=1, Tensor? weight=None, int reduction=Mean) -> Tensor", 958},
      {"aten::multi_margin_loss_backward(Tensor grad_output, Tensor self, Tensor target, Scalar p, Scalar margin, Tensor? weight=None, int reduction=Mean, *, Tensor(a!) grad_input) -> Tensor(a!)", 959},
      {"aten::multi_margin_loss_backward(Tensor grad_output, Tensor self, Tensor target, Scalar p, Scalar margin, Tensor? weight=None, int reduction=Mean) -> Tensor", 960},
      {"aten::multilabel_margin_loss(Tensor self, Tensor target, int reduction=Mean, *, Tensor(a!) out) -> Tensor(a!)", 961},
      {"aten::multilabel_margin_loss(Tensor self, Tensor target, int reduction=Mean) -> Tensor", 962},
      {"aten::multilabel_margin_loss_forward(Tensor self, Tensor target, int reduction, *, Tensor(a!) output, Tensor(b!) is_target) -> (Tensor(a!), Tensor(b!))", 963},
      {"aten::multilabel_margin_loss_forward(Tensor self, Tensor target, int reduction) -> (Tensor output, Tensor is_target)", 964},
      {"aten::multilabel_margin_loss_backward(Tensor grad_output, Tensor self, Tensor target, int reduction, Tensor is_target, *, Tensor(a!) grad_input) -> Tensor(a!)", 965},
      {"aten::multilabel_margin_loss_backward(Tensor grad_output, Tensor self, Tensor target, int reduction, Tensor is_target) -> Tensor", 966},
      {"aten::nll_loss(Tensor self, Tensor target, Tensor? weight=None, int reduction=Mean, int ignore_index=-100, *, Tensor(a!) out) -> Tensor(a!)", 967},
      {"aten::nll_loss(Tensor self, Tensor target, Tensor? weight=None, int reduction=Mean, int ignore_index=-100) -> Tensor", 968},
      {"aten::nll_loss_forward(Tensor self, Tensor target, Tensor? weight, int reduction, int ignore_index, *, Tensor(a!) output, Tensor(b!) total_weight) -> (Tensor(a!), Tensor(b!))", 969},
      {"aten::nll_loss_forward(Tensor self, Tensor target, Tensor? weight, int reduction, int ignore_index) -> (Tensor output, Tensor total_weight)", 970},
      {"aten::nll_loss_backward(Tensor grad_output, Tensor self, Tensor target, Tensor? weight, int reduction, int ignore_index, Tensor total_weight, *, Tensor(a!) grad_input) -> Tensor(a!)", 971},
      {"aten::nll_loss_backward(Tensor grad_output, Tensor self, Tensor target, Tensor? weight, int reduction, int ignore_index, Tensor total_weight) -> Tensor", 972},
      {"aten::nll_loss2d(Tensor self, Tensor target, Tensor? weight=None, int reduction=Mean, int ignore_index=-100, *, Tensor(a!) out) -> Tensor(a!)", 973},
      {"aten::nll_loss2d(Tensor self, Tensor target, Tensor? weight=None, int reduction=Mean, int ignore_index=-100) -> Tensor", 974},
      {"aten::nll_loss2d_forward(Tensor self, Tensor target, Tensor? weight, int reduction, int ignore_index, *, Tensor(a!) output, Tensor(b!) total_weight) -> (Tensor(a!), Tensor(b!))", 975},
      {"aten::nll_loss2d_forward(Tensor self, Tensor target, Tensor? weight, int reduction, int ignore_index) -> (Tensor output, Tensor total_weight)", 976},
      {"aten::nll_loss2d_backward(Tensor grad_output, Tensor self, Tensor target, Tensor? weight, int reduction, int ignore_index, Tensor total_weight, *, Tensor(a!) grad_input) -> Tensor(a!)", 977},
      {"aten::nll_loss2d_backward(Tensor grad_output, Tensor self, Tensor target, Tensor? weight, int reduction, int ignore_index, Tensor total_weight) -> Tensor", 978},
      {"aten::smooth_l1_loss(Tensor self, Tensor target, int reduction=Mean, *, Tensor(a!) out) -> Tensor(a!)", 979},
      {"aten::smooth_l1_loss(Tensor self, Tensor target, int reduction=Mean) -> Tensor", 980},
      {"aten::smooth_l1_loss_backward(Tensor grad_output, Tensor self, Tensor target, int reduction, *, Tensor(a!) grad_input) -> Tensor(a!)", 981},
      {"aten::smooth_l1_loss_backward(Tensor grad_output, Tensor self, Tensor target, int reduction) -> Tensor", 982},
      {"aten::soft_margin_loss(Tensor self, Tensor target, int reduction=Mean, *, Tensor(a!) out) -> Tensor(a!)", 983},
      {"aten::soft_margin_loss(Tensor self, Tensor target, int reduction=Mean) -> Tensor", 984},
      {"aten::soft_margin_loss_backward(Tensor grad_output, Tensor self, Tensor target, int reduction, *, Tensor(a!) grad_input) -> Tensor(a!)", 985},
      {"aten::soft_margin_loss_backward(Tensor grad_output, Tensor self, Tensor target, int reduction) -> Tensor", 986},
      {"aten::elu(Tensor self, Scalar alpha=1, Scalar scale=1, Scalar input_scale=1, *, Tensor(a!) out) -> Tensor(a!)", 987},
      {"aten::elu(Tensor self, Scalar alpha=1, Scalar scale=1, Scalar input_scale=1) -> Tensor", 988},
      {"aten::elu_backward(Tensor grad_output, Scalar alpha, Scalar scale, Scalar input_scale, Tensor output, *, Tensor(a!) grad_input) -> Tensor(a!)", 989},
      {"aten::elu_backward(Tensor grad_output, Scalar alpha, Scalar scale, Scalar input_scale, Tensor output) -> Tensor", 990},
      {"aten::elu_(Tensor(a!) self, Scalar alpha=1, Scalar scale=1, Scalar input_scale=1) -> Tensor(a!)", 991},
      {"aten::glu(Tensor self, int dim=-1, *, Tensor(a!) out) -> Tensor(a!)", 992},
      {"aten::glu(Tensor self, int dim=-1) -> Tensor", 993},
      {"aten::glu_backward(Tensor grad_output, Tensor self, int dim, *, Tensor(a!) grad_input) -> Tensor(a!)", 994},
      {"aten::glu_backward(Tensor grad_output, Tensor self, int dim) -> Tensor", 995},
      {"aten::hardtanh(Tensor self, Scalar min_val=-1, Scalar max_val=1, *, Tensor(a!) out) -> Tensor(a!)", 996},
      {"aten::hardtanh(Tensor self, Scalar min_val=-1, Scalar max_val=1) -> Tensor", 997},
      {"aten::hardtanh_backward(Tensor grad_output, Tensor self, Scalar min_val, Scalar max_val, *, Tensor(a!) grad_input) -> Tensor(a!)", 998},
      {"aten::hardtanh_backward(Tensor grad_output, Tensor self, Scalar min_val, Scalar max_val) -> Tensor", 999},
      {"aten::hardtanh_(Tensor(a!) self, Scalar min_val=-1, Scalar max_val=1) -> Tensor(a!)", 1000},
      {"aten::leaky_relu(Tensor self, Scalar negative_slope=0.01, *, Tensor(a!) out) -> Tensor(a!)", 1001},
      {"aten::leaky_relu(Tensor self, Scalar negative_slope=0.01) -> Tensor", 1002},
      {"aten::leaky_relu_backward(Tensor grad_output, Tensor self, Scalar negative_slope, *, Tensor(a!) grad_input) -> Tensor(a!)", 1003},
      {"aten::leaky_relu_backward(Tensor grad_output, Tensor self, Scalar negative_slope) -> Tensor", 1004},
      {"aten::leaky_relu_(Tensor(a!) self, Scalar negative_slope=0.01) -> Tensor(a!)", 1005},
      {"aten::log_sigmoid(Tensor self, *, Tensor(a!) out) -> Tensor(a!)", 1006},
      {"aten::log_sigmoid(Tensor self) -> Tensor", 1007},
      {"aten::log_sigmoid_forward(Tensor self, *, Tensor(a!) output, Tensor(b!) buffer) -> (Tensor(a!), Tensor(b!))", 1008},
      {"aten::log_sigmoid_forward(Tensor self) -> (Tensor output, Tensor buffer)", 1009},
      {"aten::log_sigmoid_backward(Tensor grad_output, Tensor self, Tensor buffer, *, Tensor(a!) grad_input) -> Tensor(a!)", 1010},
      {"aten::log_sigmoid_backward(Tensor grad_output, Tensor self, Tensor buffer) -> Tensor", 1011},
      {"aten::rrelu_with_noise(Tensor self, Tensor noise, Scalar lower=0.125, Scalar upper=0.3333333333333333, bool training=False, Generator? generator=None, *, Tensor(a!) out) -> Tensor(a!)", 1012},
      {"aten::rrelu_with_noise(Tensor self, Tensor noise, Scalar lower=0.125, Scalar upper=0.3333333333333333, bool training=False, Generator? generator=None) -> Tensor", 1013},
      {"aten::rrelu_with_noise_backward(Tensor grad_output, Tensor self, Tensor noise, Scalar lower, Scalar upper, bool training, *, Tensor(a!) grad_input) -> Tensor(a!)", 1014},
      {"aten::rrelu_with_noise_backward(Tensor grad_output, Tensor self, Tensor noise, Scalar lower, Scalar upper, bool training) -> Tensor", 1015},
      {"aten::rrelu_with_noise_(Tensor(a!) self, Tensor noise, Scalar lower=0.125, Scalar upper=0.3333333333333333, bool training=False, Generator? generator=None) -> Tensor(a!)", 1016},
      {"aten::softplus(Tensor self, Scalar beta=1, Scalar threshold=20, *, Tensor(a!) out) -> Tensor(a!)", 1017},
      {"aten::softplus(Tensor self, Scalar beta=1, Scalar threshold=20) -> Tensor", 1018},
      {"aten::softplus_backward(Tensor grad_output, Tensor self, Scalar beta, Scalar threshold, Tensor output, *, Tensor(a!) grad_input) -> Tensor(a!)", 1019},
      {"aten::softplus_backward(Tensor grad_output, Tensor self, Scalar beta, Scalar threshold, Tensor output) -> Tensor", 1020},
      {"aten::softshrink(Tensor self, Scalar lambd=0.5, *, Tensor(a!) out) -> Tensor(a!)", 1021},
      {"aten::softshrink(Tensor self, Scalar lambd=0.5) -> Tensor", 1022},
      {"aten::softshrink_backward(Tensor grad_output, Tensor self, Scalar lambd, *, Tensor(a!) grad_input) -> Tensor(a!)", 1023},
      {"aten::softshrink_backward(Tensor grad_output, Tensor self, Scalar lambd) -> Tensor", 1024},
      {"aten::adaptive_avg_pool2d(Tensor self, int[2] output_size, *, Tensor(a!) out) -> Tensor(a!)", 1025},
      {"aten::adaptive_avg_pool2d(Tensor self, int[2] output_size) -> Tensor", 1026},
      {"aten::mkldnn_adaptive_avg_pool2d(Tensor self, int[2] output_size) -> Tensor", 1027},
      {"aten::_adaptive_avg_pool2d(Tensor self, int[2] output_size) -> Tensor", 1028},
      {"aten::_adaptive_avg_pool2d_backward(Tensor grad_output, Tensor self) -> Tensor", 1029},
      {"aten::adaptive_avg_pool3d(Tensor self, int[3] output_size, *, Tensor(a!) out) -> Tensor(a!)", 1030},
      {"aten::adaptive_avg_pool3d(Tensor self, int[3] output_size) -> Tensor", 1031},
      {"aten::adaptive_avg_pool3d_backward(Tensor grad_output, Tensor self, *, Tensor(a!) grad_input) -> Tensor(a!)", 1032},
      {"aten::adaptive_avg_pool3d_backward(Tensor grad_output, Tensor self) -> Tensor", 1033},
      {"aten::adaptive_max_pool2d(Tensor self, int[2] output_size, *, Tensor(a!) out, Tensor(b!) indices) -> (Tensor(a!), Tensor(b!))", 1034},
      {"aten::adaptive_max_pool2d(Tensor self, int[2] output_size) -> (Tensor, Tensor)", 1035},
      {"aten::adaptive_max_pool2d_backward(Tensor grad_output, Tensor self, Tensor indices, *, Tensor(a!) grad_input) -> Tensor(a!)", 1036},
      {"aten::adaptive_max_pool2d_backward(Tensor grad_output, Tensor self, Tensor indices) -> Tensor", 1037},
      {"aten::adaptive_max_pool3d(Tensor self, int[3] output_size, *, Tensor(a!) out, Tensor(b!) indices) -> (Tensor(a!), Tensor(b!))", 1038},
      {"aten::adaptive_max_pool3d(Tensor self, int[3] output_size) -> (Tensor, Tensor)", 1039},
      {"aten::adaptive_max_pool3d_backward(Tensor grad_output, Tensor self, Tensor indices, *, Tensor(a!) grad_input) -> Tensor(a!)", 1040},
      {"aten::adaptive_max_pool3d_backward(Tensor grad_output, Tensor self, Tensor indices) -> Tensor", 1041},
      {"aten::avg_pool2d(Tensor self, int[2] kernel_size, int[2] stride=[], int[2] padding=0, bool ceil_mode=False, bool count_include_pad=True, *, Tensor(a!) out) -> Tensor(a!)", 1042},
      {"aten::avg_pool2d(Tensor self, int[2] kernel_size, int[2] stride=[], int[2] padding=0, bool ceil_mode=False, bool count_include_pad=True) -> Tensor", 1043},
      {"aten::avg_pool2d_backward(Tensor grad_output, Tensor self, int[2] kernel_size, int[2] stride, int[2] padding, bool ceil_mode, bool count_include_pad, *, Tensor(a!) grad_input) -> Tensor(a!)", 1044},
      {"aten::avg_pool2d_backward(Tensor grad_output, Tensor self, int[2] kernel_size, int[2] stride, int[2] padding, bool ceil_mode, bool count_include_pad) -> Tensor", 1045},
      {"aten::avg_pool3d(Tensor self, int[3] kernel_size, int[3] stride=[], int[3] padding=0, bool ceil_mode=False, bool count_include_pad=True, *, Tensor(a!) out) -> Tensor(a!)", 1046},
      {"aten::avg_pool3d(Tensor self, int[3] kernel_size, int[3] stride=[], int[3] padding=0, bool ceil_mode=False, bool count_include_pad=True) -> Tensor", 1047},
      {"aten::avg_pool3d_backward(Tensor grad_output, Tensor self, int[3] kernel_size, int[3] stride, int[3] padding, bool ceil_mode, bool count_include_pad, *, Tensor(a!) grad_input) -> Tensor(a!)", 1048},
      {"aten::avg_pool3d_backward(Tensor grad_output, Tensor self, int[3] kernel_size, int[3] stride, int[3] padding, bool ceil_mode, bool count_include_pad) -> Tensor", 1049},
      {"aten::fractional_max_pool2d(Tensor self, int[2] kernel_size, int[2] output_size, Tensor random_samples, *, Tensor(a!) output, Tensor(b!) indices) -> (Tensor(a!), Tensor(b!))", 1050},
      {"aten::fractional_max_pool2d(Tensor self, int[2] kernel_size, int[2] output_size, Tensor random_samples) -> (Tensor, Tensor)", 1051},
      {"aten::fractional_max_pool2d_backward(Tensor grad_output, Tensor self, int[2] kernel_size, int[2] output_size, Tensor indices, *, Tensor(a!) grad_input) -> Tensor(a!)", 1052},
      {"aten::fractional_max_pool2d_backward(Tensor grad_output, Tensor self, int[2] kernel_size, int[2] output_size, Tensor indices) -> Tensor", 1053},
      {"aten::fractional_max_pool3d(Tensor self, int[3] kernel_size, int[3] output_size, Tensor random_samples, *, Tensor(a!) output, Tensor(b!) indices) -> (Tensor(a!), Tensor(b!))", 1054},
      {"aten::fractional_max_pool3d(Tensor self, int[3] kernel_size, int[3] output_size, Tensor random_samples) -> (Tensor, Tensor)", 1055},
      {"aten::fractional_max_pool3d_backward(Tensor grad_output, Tensor self, int[3] kernel_size, int[3] output_size, Tensor indices, *, Tensor(a!) grad_input) -> Tensor(a!)", 1056},
      {"aten::fractional_max_pool3d_backward(Tensor grad_output, Tensor self, int[3] kernel_size, int[3] output_size, Tensor indices) -> Tensor", 1057},
      {"aten::max_pool2d_with_indices(Tensor self, int[2] kernel_size, int[2] stride=[], int[2] padding=0, int[2] dilation=1, bool ceil_mode=False, *, Tensor(a!) out, Tensor(b!) indices) -> (Tensor(a!), Tensor(b!))", 1058},
      {"aten::max_pool2d_with_indices(Tensor self, int[2] kernel_size, int[2] stride=[], int[2] padding=0, int[2] dilation=1, bool ceil_mode=False) -> (Tensor, Tensor)", 1059},
      {"aten::max_pool2d_with_indices_backward(Tensor grad_output, Tensor self, int[2] kernel_size, int[2] stride, int[2] padding, int[2] dilation, bool ceil_mode, Tensor indices, *, Tensor(a!) grad_input) -> Tensor(a!)", 1060},
      {"aten::max_pool2d_with_indices_backward(Tensor grad_output, Tensor self, int[2] kernel_size, int[2] stride, int[2] padding, int[2] dilation, bool ceil_mode, Tensor indices) -> Tensor", 1061},
      {"aten::max_pool3d_with_indices(Tensor self, int[3] kernel_size, int[3] stride=[], int[3] padding=0, int[3] dilation=1, bool ceil_mode=False, *, Tensor(a!) out, Tensor(b!) indices) -> (Tensor(a!), Tensor(b!))", 1062},
      {"aten::max_pool3d_with_indices(Tensor self, int[3] kernel_size, int[3] stride=[], int[3] padding=0, int[3] dilation=1, bool ceil_mode=False) -> (Tensor, Tensor)", 1063},
      {"aten::max_pool3d_with_indices_backward(Tensor grad_output, Tensor self, int[3] kernel_size, int[3] stride, int[3] padding, int[3] dilation, bool ceil_mode, Tensor indices, *, Tensor(a!) grad_input) -> Tensor(a!)", 1064},
      {"aten::max_pool3d_with_indices_backward(Tensor grad_output, Tensor self, int[3] kernel_size, int[3] stride, int[3] padding, int[3] dilation, bool ceil_mode, Tensor indices) -> Tensor", 1065},
      {"aten::max_unpool2d(Tensor self, Tensor indices, int[2] output_size, *, Tensor(a!) out) -> Tensor(a!)", 1066},
      {"aten::max_unpool2d(Tensor self, Tensor indices, int[2] output_size) -> Tensor", 1067},
      {"aten::max_unpool2d_backward(Tensor grad_output, Tensor self, Tensor indices, int[2] output_size, *, Tensor(a!) grad_input) -> Tensor(a!)", 1068},
      {"aten::max_unpool2d_backward(Tensor grad_output, Tensor self, Tensor indices, int[2] output_size) -> Tensor", 1069},
      {"aten::max_unpool3d(Tensor self, Tensor indices, int[3] output_size, int[3] stride, int[3] padding, *, Tensor(a!) out) -> Tensor(a!)", 1070},
      {"aten::max_unpool3d(Tensor self, Tensor indices, int[3] output_size, int[3] stride, int[3] padding) -> Tensor", 1071},
      {"aten::max_unpool3d_backward(Tensor grad_output, Tensor self, Tensor indices, int[3] output_size, int[3] stride, int[3] padding, *, Tensor(a!) grad_input) -> Tensor(a!)", 1072},
      {"aten::max_unpool3d_backward(Tensor grad_output, Tensor self, Tensor indices, int[3] output_size, int[3] stride, int[3] padding) -> Tensor", 1073},
      {"aten::reflection_pad1d(Tensor self, int[2] padding, *, Tensor(a!) out) -> Tensor(a!)", 1074},
      {"aten::reflection_pad1d(Tensor self, int[2] padding) -> Tensor", 1075},
      {"aten::reflection_pad1d_backward(Tensor grad_output, Tensor self, int[2] padding, *, Tensor(a!) grad_input) -> Tensor(a!)", 1076},
      {"aten::reflection_pad1d_backward(Tensor grad_output, Tensor self, int[2] padding) -> Tensor", 1077},
      {"aten::reflection_pad2d(Tensor self, int[4] padding, *, Tensor(a!) out) -> Tensor(a!)", 1078},
      {"aten::reflection_pad2d(Tensor self, int[4] padding) -> Tensor", 1079},
      {"aten::reflection_pad2d_backward(Tensor grad_output, Tensor self, int[4] padding, *, Tensor(a!) grad_input) -> Tensor(a!)", 1080},
      {"aten::reflection_pad2d_backward(Tensor grad_output, Tensor self, int[4] padding) -> Tensor", 1081},
      {"aten::replication_pad1d(Tensor self, int[2] padding, *, Tensor(a!) out) -> Tensor(a!)", 1082},
      {"aten::replication_pad1d(Tensor self, int[2] padding) -> Tensor", 1083},
      {"aten::replication_pad1d_backward(Tensor grad_output, Tensor self, int[2] padding, *, Tensor(a!) grad_input) -> Tensor(a!)", 1084},
      {"aten::replication_pad1d_backward(Tensor grad_output, Tensor self, int[2] padding) -> Tensor", 1085},
      {"aten::replication_pad2d(Tensor self, int[4] padding, *, Tensor(a!) out) -> Tensor(a!)", 1086},
      {"aten::replication_pad2d(Tensor self, int[4] padding) -> Tensor", 1087},
      {"aten::replication_pad2d_backward(Tensor grad_output, Tensor self, int[4] padding, *, Tensor(a!) grad_input) -> Tensor(a!)", 1088},
      {"aten::replication_pad2d_backward(Tensor grad_output, Tensor self, int[4] padding) -> Tensor", 1089},
      {"aten::replication_pad3d(Tensor self, int[6] padding, *, Tensor(a!) out) -> Tensor(a!)", 1090},
      {"aten::replication_pad3d(Tensor self, int[6] padding) -> Tensor", 1091},
      {"aten::replication_pad3d_backward(Tensor grad_output, Tensor self, int[6] padding, *, Tensor(a!) grad_input) -> Tensor(a!)", 1092},
      {"aten::replication_pad3d_backward(Tensor grad_output, Tensor self, int[6] padding) -> Tensor", 1093},
      {"aten::upsample_linear1d(Tensor self, int[1] output_size, bool align_corners, *, Tensor(a!) out) -> Tensor(a!)", 1094},
      {"aten::upsample_linear1d(Tensor self, int[1] output_size, bool align_corners) -> Tensor", 1095},
      {"aten::upsample_linear1d_backward(Tensor grad_output, int[1] output_size, int[3] input_size, bool align_corners, *, Tensor(a!) grad_input) -> Tensor(a!)", 1096},
      {"aten::upsample_linear1d_backward(Tensor grad_output, int[1] output_size, int[3] input_size, bool align_corners) -> Tensor", 1097},
      {"aten::upsample_bilinear2d(Tensor self, int[2] output_size, bool align_corners, *, Tensor(a!) out) -> Tensor(a!)", 1098},
      {"aten::upsample_bilinear2d(Tensor self, int[2] output_size, bool align_corners) -> Tensor", 1099},
      {"aten::upsample_bilinear2d_backward(Tensor grad_output, int[2] output_size, int[4] input_size, bool align_corners, *, Tensor(a!) grad_input) -> Tensor(a!)", 1100},
      {"aten::upsample_bilinear2d_backward(Tensor grad_output, int[2] output_size, int[4] input_size, bool align_corners) -> Tensor", 1101},
      {"aten::upsample_bicubic2d(Tensor self, int[2] output_size, bool align_corners, *, Tensor(a!) out) -> Tensor(a!)", 1102},
      {"aten::upsample_bicubic2d(Tensor self, int[2] output_size, bool align_corners) -> Tensor", 1103},
      {"aten::upsample_bicubic2d_backward(Tensor grad_output, int[2] output_size, int[4] input_size, bool align_corners, *, Tensor(a!) grad_input) -> Tensor(a!)", 1104},
      {"aten::upsample_bicubic2d_backward(Tensor grad_output, int[2] output_size, int[4] input_size, bool align_corners) -> Tensor", 1105},
      {"aten::upsample_trilinear3d(Tensor self, int[3] output_size, bool align_corners, *, Tensor(a!) out) -> Tensor(a!)", 1106},
      {"aten::upsample_trilinear3d(Tensor self, int[3] output_size, bool align_corners) -> Tensor", 1107},
      {"aten::upsample_trilinear3d_backward(Tensor grad_output, int[3] output_size, int[5] input_size, bool align_corners, *, Tensor(a!) grad_input) -> Tensor(a!)", 1108},
      {"aten::upsample_trilinear3d_backward(Tensor grad_output, int[3] output_size, int[5] input_size, bool align_corners) -> Tensor", 1109},
      {"aten::upsample_nearest1d(Tensor self, int[1] output_size, *, Tensor(a!) out) -> Tensor(a!)", 1110},
      {"aten::upsample_nearest1d(Tensor self, int[1] output_size) -> Tensor", 1111},
      {"aten::upsample_nearest1d_backward(Tensor grad_output, int[1] output_size, int[3] input_size, *, Tensor(a!) grad_input) -> Tensor(a!)", 1112},
      {"aten::upsample_nearest1d_backward(Tensor grad_output, int[1] output_size, int[3] input_size) -> Tensor", 1113},
      {"aten::upsample_nearest2d(Tensor self, int[2] output_size, *, Tensor(a!) out) -> Tensor(a!)", 1114},
      {"aten::upsample_nearest2d(Tensor self, int[2] output_size) -> Tensor", 1115},
      {"aten::upsample_nearest2d_backward(Tensor grad_output, int[2] output_size, int[4] input_size, *, Tensor(a!) grad_input) -> Tensor(a!)", 1116},
      {"aten::upsample_nearest2d_backward(Tensor grad_output, int[2] output_size, int[4] input_size) -> Tensor", 1117},
      {"aten::upsample_nearest3d(Tensor self, int[3] output_size, *, Tensor(a!) out) -> Tensor(a!)", 1118},
      {"aten::upsample_nearest3d(Tensor self, int[3] output_size) -> Tensor", 1119},
      {"aten::upsample_nearest3d_backward(Tensor grad_output, int[3] output_size, int[5] input_size, *, Tensor(a!) grad_input) -> Tensor(a!)", 1120},
      {"aten::upsample_nearest3d_backward(Tensor grad_output, int[3] output_size, int[5] input_size) -> Tensor", 1121},
      {"aten::sigmoid_backward(Tensor grad_output, Tensor output, *, Tensor(a!) grad_input) -> Tensor(a!)", 1122},
      {"aten::sigmoid_backward(Tensor grad_output, Tensor output) -> Tensor", 1123},
      {"aten::tanh_backward(Tensor grad_output, Tensor output, *, Tensor(a!) grad_input) -> Tensor(a!)", 1124},
      {"aten::tanh_backward(Tensor grad_output, Tensor output) -> Tensor", 1125},
      {"aten::thnn_conv_transpose2d(Tensor self, Tensor weight, int[2] kernel_size, Tensor? bias=None, int[2] stride=1, int[2] padding=0, int[2] output_padding=0, int[2] dilation=1, *, Tensor(a!) out) -> Tensor(a!)", 1126},
      {"aten::thnn_conv_transpose2d(Tensor self, Tensor weight, int[2] kernel_size, Tensor? bias=None, int[2] stride=1, int[2] padding=0, int[2] output_padding=0, int[2] dilation=1) -> Tensor", 1127},
      {"aten::thnn_conv_transpose2d_forward(Tensor self, Tensor weight, int[2] kernel_size, Tensor? bias, int[2] stride, int[2] padding, int[2] output_padding, int[2] dilation, *, Tensor(a!) output, Tensor(b!) columns, Tensor(c!) ones) -> (Tensor(a!), Tensor(b!), Tensor(c!))", 1128},
      {"aten::thnn_conv_transpose2d_forward(Tensor self, Tensor weight, int[2] kernel_size, Tensor? bias, int[2] stride, int[2] padding, int[2] output_padding, int[2] dilation) -> (Tensor output, Tensor columns, Tensor ones)", 1129},
      {"aten::thnn_conv_transpose2d_backward(Tensor grad_output, Tensor self, Tensor weight, int[2] kernel_size, int[2] stride, int[2] padding, int[2] output_padding, int[2] dilation, Tensor columns, Tensor ones, *, Tensor?(a!) grad_input, Tensor?(b!) grad_weight, Tensor?(c!) grad_bias) -> (Tensor(a!), Tensor(b!), Tensor(c!))", 1130},
      {"aten::thnn_conv_transpose2d_backward(Tensor grad_output, Tensor self, Tensor weight, int[2] kernel_size, int[2] stride, int[2] padding, int[2] output_padding, int[2] dilation, Tensor columns, Tensor ones, bool[3] output_mask) -> (Tensor grad_input, Tensor grad_weight, Tensor grad_bias)", 1131},
      {"aten::thnn_conv_transpose3d(Tensor self, Tensor weight, int[3] kernel_size, Tensor? bias=None, int[3] stride=1, int[3] padding=0, int[3] output_padding=0, int[3] dilation=1, *, Tensor(a!) out) -> Tensor(a!)", 1132},
      {"aten::thnn_conv_transpose3d(Tensor self, Tensor weight, int[3] kernel_size, Tensor? bias=None, int[3] stride=1, int[3] padding=0, int[3] output_padding=0, int[3] dilation=1) -> Tensor", 1133},
      {"aten::thnn_conv_transpose3d_forward(Tensor self, Tensor weight, int[3] kernel_size, Tensor? bias, int[3] stride, int[3] padding, int[3] output_padding, int[3] dilation, *, Tensor(a!) output, Tensor(b!) finput, Tensor(c!) fgrad_input) -> (Tensor(a!), Tensor(b!), Tensor(c!))", 1134},
      {"aten::thnn_conv_transpose3d_forward(Tensor self, Tensor weight, int[3] kernel_size, Tensor? bias, int[3] stride, int[3] padding, int[3] output_padding, int[3] dilation) -> (Tensor output, Tensor finput, Tensor fgrad_input)", 1135},
      {"aten::thnn_conv_transpose3d_backward(Tensor grad_output, Tensor self, Tensor weight, int[3] kernel_size, int[3] stride, int[3] padding, int[3] output_padding, int[3] dilation, Tensor finput, Tensor fgrad_input, *, Tensor?(a!) grad_input, Tensor?(b!) grad_weight, Tensor?(c!) grad_bias) -> (Tensor(a!), Tensor(b!), Tensor(c!))", 1136},
      {"aten::thnn_conv_transpose3d_backward(Tensor grad_output, Tensor self, Tensor weight, int[3] kernel_size, int[3] stride, int[3] padding, int[3] output_padding, int[3] dilation, Tensor finput, Tensor fgrad_input, bool[3] output_mask) -> (Tensor grad_input, Tensor grad_weight, Tensor grad_bias)", 1137},
      {"aten::thnn_conv2d(Tensor self, Tensor weight, int[2] kernel_size, Tensor? bias=None, int[2] stride=1, int[2] padding=0, *, Tensor(a!) out) -> Tensor(a!)", 1138},
      {"aten::thnn_conv2d(Tensor self, Tensor weight, int[2] kernel_size, Tensor? bias=None, int[2] stride=1, int[2] padding=0) -> Tensor", 1139},
      {"aten::thnn_conv2d_forward(Tensor self, Tensor weight, int[2] kernel_size, Tensor? bias, int[2] stride, int[2] padding, *, Tensor(a!) output, Tensor(b!) finput, Tensor(c!) fgrad_input) -> (Tensor(a!), Tensor(b!), Tensor(c!))", 1140},
      {"aten::thnn_conv2d_forward(Tensor self, Tensor weight, int[2] kernel_size, Tensor? bias, int[2] stride, int[2] padding) -> (Tensor output, Tensor finput, Tensor fgrad_input)", 1141},
      {"aten::thnn_conv2d_backward(Tensor grad_output, Tensor self, Tensor weight, int[2] kernel_size, int[2] stride, int[2] padding, Tensor finput, Tensor fgrad_input, *, Tensor?(a!) grad_input, Tensor?(b!) grad_weight, Tensor?(c!) grad_bias) -> (Tensor(a!), Tensor(b!), Tensor(c!))", 1142},
      {"aten::thnn_conv2d_backward(Tensor grad_output, Tensor self, Tensor weight, int[2] kernel_size, int[2] stride, int[2] padding, Tensor finput, Tensor fgrad_input, bool[3] output_mask) -> (Tensor grad_input, Tensor grad_weight, Tensor grad_bias)", 1143},
      {"aten::thnn_conv_depthwise2d(Tensor self, Tensor weight, int[2] kernel_size, Tensor? bias=None, int[2] stride=1, int[2] padding=0, int[2] dilation=1, *, Tensor(a!) out) -> Tensor(a!)", 1144},
      {"aten::thnn_conv_depthwise2d(Tensor self, Tensor weight, int[2] kernel_size, Tensor? bias=None, int[2] stride=1, int[2] padding=0, int[2] dilation=1) -> Tensor", 1145},
      {"aten::thnn_conv_depthwise2d_forward(Tensor self, Tensor weight, int[2] kernel_size, Tensor? bias, int[2] stride, int[2] padding, int[2] dilation, *, Tensor(a!) out) -> Tensor(a!)", 1146},
      {"aten::thnn_conv_depthwise2d_forward(Tensor self, Tensor weight, int[2] kernel_size, Tensor? bias, int[2] stride, int[2] padding, int[2] dilation) -> Tensor", 1147},
      {"aten::thnn_conv_depthwise2d_backward(Tensor grad_output, Tensor self, Tensor weight, int[2] kernel_size, int[2] stride, int[2] padding, int[2] dilation, *, Tensor?(a!) grad_input, Tensor?(b!) grad_weight) -> (Tensor(a!), Tensor(b!))", 1148},
      {"aten::thnn_conv_depthwise2d_backward(Tensor grad_output, Tensor self, Tensor weight, int[2] kernel_size, int[2] stride, int[2] padding, int[2] dilation, bool[2] output_mask) -> (Tensor grad_input, Tensor grad_weight)", 1149},
      {"aten::thnn_conv3d(Tensor self, Tensor weight, int[3] kernel_size, Tensor? bias=None, int[3] stride=1, int[3] padding=0, *, Tensor(a!) out) -> Tensor(a!)", 1150},
      {"aten::thnn_conv3d(Tensor self, Tensor weight, int[3] kernel_size, Tensor? bias=None, int[3] stride=1, int[3] padding=0) -> Tensor", 1151},
      {"aten::thnn_conv3d_forward(Tensor self, Tensor weight, int[3] kernel_size, Tensor? bias, int[3] stride, int[3] padding, *, Tensor(a!) output, Tensor(b!) finput, Tensor(c!) fgrad_input) -> (Tensor(a!), Tensor(b!), Tensor(c!))", 1152},
      {"aten::thnn_conv3d_forward(Tensor self, Tensor weight, int[3] kernel_size, Tensor? bias, int[3] stride, int[3] padding) -> (Tensor output, Tensor finput, Tensor fgrad_input)", 1153},
      {"aten::thnn_conv3d_backward(Tensor grad_output, Tensor self, Tensor weight, int[3] kernel_size, int[3] stride, int[3] padding, Tensor finput, Tensor fgrad_input, *, Tensor?(a!) grad_input, Tensor?(b!) grad_weight, Tensor?(c!) grad_bias) -> (Tensor(a!), Tensor(b!), Tensor(c!))", 1154},
      {"aten::thnn_conv3d_backward(Tensor grad_output, Tensor self, Tensor weight, int[3] kernel_size, int[3] stride, int[3] padding, Tensor finput, Tensor fgrad_input, bool[3] output_mask) -> (Tensor grad_input, Tensor grad_weight, Tensor grad_bias)", 1155},
      {"aten::thnn_conv_dilated2d(Tensor self, Tensor weight, int[2] kernel_size, Tensor? bias=None, int[2] stride=1, int[2] padding=0, int[2] dilation=1, *, Tensor(a!) out) -> Tensor(a!)", 1156},
      {"aten::thnn_conv_dilated2d(Tensor self, Tensor weight, int[2] kernel_size, Tensor? bias=None, int[2] stride=1, int[2] padding=0, int[2] dilation=1) -> Tensor", 1157},
      {"aten::thnn_conv_dilated2d_forward(Tensor self, Tensor weight, int[2] kernel_size, Tensor? bias, int[2] stride, int[2] padding, int[2] dilation, *, Tensor(a!) output, Tensor(b!) columns, Tensor(c!) ones) -> (Tensor(a!), Tensor(b!), Tensor(c!))", 1158},
      {"aten::thnn_conv_dilated2d_forward(Tensor self, Tensor weight, int[2] kernel_size, Tensor? bias, int[2] stride, int[2] padding, int[2] dilation) -> (Tensor output, Tensor columns, Tensor ones)", 1159},
      {"aten::thnn_conv_dilated2d_backward(Tensor grad_output, Tensor self, Tensor weight, int[2] kernel_size, int[2] stride, int[2] padding, int[2] dilation, Tensor columns, Tensor ones, *, Tensor?(a!) grad_input, Tensor?(b!) grad_weight, Tensor?(c!) grad_bias) -> (Tensor(a!), Tensor(b!), Tensor(c!))", 1160},
      {"aten::thnn_conv_dilated2d_backward(Tensor grad_output, Tensor self, Tensor weight, int[2] kernel_size, int[2] stride, int[2] padding, int[2] dilation, Tensor columns, Tensor ones, bool[3] output_mask) -> (Tensor grad_input, Tensor grad_weight, Tensor grad_bias)", 1161},
      {"aten::thnn_conv_dilated3d(Tensor self, Tensor weight, int[3] kernel_size, Tensor? bias=None, int[3] stride=1, int[3] padding=0, int[3] dilation=1, *, Tensor(a!) out) -> Tensor(a!)", 1162},
      {"aten::thnn_conv_dilated3d(Tensor self, Tensor weight, int[3] kernel_size, Tensor? bias=None, int[3] stride=1, int[3] padding=0, int[3] dilation=1) -> Tensor", 1163},
      {"aten::thnn_conv_dilated3d_forward(Tensor self, Tensor weight, int[3] kernel_size, Tensor? bias, int[3] stride, int[3] padding, int[3] dilation, *, Tensor(a!) output, Tensor(b!) columns, Tensor(c!) ones) -> (Tensor(a!), Tensor(b!), Tensor(c!))", 1164},
      {"aten::thnn_conv_dilated3d_forward(Tensor self, Tensor weight, int[3] kernel_size, Tensor? bias, int[3] stride, int[3] padding, int[3] dilation) -> (Tensor output, Tensor columns, Tensor ones)", 1165},
      {"aten::thnn_conv_dilated3d_backward(Tensor grad_output, Tensor self, Tensor weight, int[3] kernel_size, int[3] stride, int[3] padding, int[] dilation, Tensor columns, Tensor ones, *, Tensor?(a!) grad_input, Tensor?(b!) grad_weight, Tensor?(c!) grad_bias) -> (Tensor(a!), Tensor(b!), Tensor(c!))", 1166},
      {"aten::thnn_conv_dilated3d_backward(Tensor grad_output, Tensor self, Tensor weight, int[3] kernel_size, int[3] stride, int[3] padding, int[3] dilation, Tensor columns, Tensor ones, bool[3] output_mask) -> (Tensor grad_input, Tensor grad_weight, Tensor grad_bias)", 1167},
      {"aten::thnn_col2im(Tensor self, int[2] output_size, int[2] kernel_size, int[2] dilation, int[2] padding, int[2] stride) -> Tensor", 1168},
      {"aten::thnn_col2im_backward(Tensor grad_output, int[2] kernel_size, int[2] dilation, int[2] padding, int[2] stride) -> Tensor", 1169},
      {"aten::thnn_im2col(Tensor self, int[2] kernel_size, int[2] dilation, int[2] padding, int[2] stride) -> Tensor", 1170},
      {"aten::thnn_im2col_backward(Tensor grad_output, int[2] input_size, int[2] kernel_size, int[2] dilation, int[2] padding, int[2] stride) -> Tensor", 1171},
    };
    return schema_to_id[schema];
  }

  void** getFunctionTable(Backend backend) {
    static void* function_table[static_cast<int64_t>(Backend::NumOptions)][1172];
    return function_table[static_cast<int64_t>(backend)];
  }
  void** getWrapperTable() {
    static void* wrapper_table[1172];
    return wrapper_table;
  }
};

CAFFE2_API ATenDispatch& globalATenDispatch();

} // namespace at
